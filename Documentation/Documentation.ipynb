{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation\n",
    "\n",
    "This notebook serves to give an example of how to use functions in this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import re\n",
    "from os import stat\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "from os.path import exists\n",
    "from netCDF4 import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url, params={}, output=None):\n",
    "    \"\"\"Takes a url ar well as optional wikipedia parameters and returnd a regex object containgin a string version of the html\n",
    "\n",
    "    Args:\n",
    "        url (str)       : A String of a wikipedia url\n",
    "        params (str)    : (Optional) Wikipedia parameters, will change regex output\n",
    "        output (str)    : (Optional) If passed, the html is saved to a file with $output as name\n",
    "\n",
    "    Writes:\n",
    "        html            :(Optional) Writes string from regex object to file if filname is passed\n",
    "\n",
    "    Returns:\n",
    "        regex object(obj): A regex object containing the html code as a string\n",
    "    \"\"\"\n",
    "    r = req.get(url, params=params)\n",
    "\n",
    "    if output != None:\n",
    "        HTML_file = open(output, \"w\", encoding=\"utf-8\")\n",
    "        HTML_file.write(\"<!-- \" + r.url + \"--> \\n\")\n",
    "        HTML_file.write(r.text)\n",
    "        HTML_file.close()\n",
    "\n",
    "    # print(r.url)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "(267, 12) 267\n",
      "[160]\n",
      "AK 0\n",
      "AK 1\n",
      "AK 2\n",
      "AK 3\n",
      "AK 4\n",
      "AK 5\n",
      "AK 6\n",
      "AK 7\n",
      "AK 8\n",
      "AK 9\n",
      "AK 10\n",
      "AK 11\n",
      "AK 12\n",
      "AK 13\n",
      "AK 14\n",
      "AK 15\n",
      "AK 16\n",
      "AK 17\n",
      "AK 18\n",
      "AK 19\n",
      "AK 20\n",
      "AK 21\n",
      "AK 22\n",
      "AK 23\n",
      "AK 24\n",
      "AK 25\n",
      "AK 26\n",
      "AK 27\n",
      "AK 28\n",
      "AK 29\n",
      "AK 30\n",
      "AK 31\n",
      "AK 32\n",
      "AK 33\n",
      "AK 34\n",
      "AK 35\n",
      "AK 36\n",
      "AK 37\n",
      "AK 38\n",
      "AK 39\n",
      "AK 40\n",
      "AK 41\n",
      "AK 42\n",
      "AK 43\n",
      "AK 44\n",
      "AK 45\n",
      "AK 46\n",
      "AK 47\n",
      "AK 48\n",
      "AK 49\n",
      "AK 50\n",
      "AK 51\n",
      "AK 52\n",
      "AK 53\n",
      "AK 54\n",
      "AK 55\n",
      "AK 56\n",
      "AK 57\n",
      "57 Didnt work, nan\n",
      "AK 58\n",
      "AK 59\n",
      "AK 60\n",
      "AK 61\n",
      "AK 62\n",
      "AK 63\n",
      "AK 64\n",
      "AK 65\n",
      "AK 66\n",
      "AK 67\n",
      "AK 68\n",
      "AK 69\n",
      "69 Didnt work, nan\n",
      "AK 70\n",
      "AK 71\n",
      "AK 72\n",
      "AK 73\n",
      "AK 74\n",
      "AK 75\n",
      "AK 76\n",
      "AK 77\n",
      "AK 78\n",
      "AK 79\n",
      "AK 80\n",
      "AK 81\n",
      "AK 82\n",
      "AK 83\n",
      "AK 84\n",
      "AK 85\n",
      "AK 86\n",
      "AK 87\n",
      "87 Didnt work, nan\n",
      "AK 88\n",
      "88 Didnt work, nan\n",
      "AK 89\n",
      "AK 90\n",
      "AK 91\n",
      "AK 92\n",
      "AK 93\n",
      "AK 94\n",
      "AK 95\n",
      "AK 96\n",
      "AK 97\n",
      "97 Didnt work, nan\n",
      "AK 98\n",
      "AK 99\n",
      "99 Didnt work, 34J07S\n",
      "AK 100\n",
      "AK 101\n",
      "AK 102\n",
      "AK 103\n",
      "AK 104\n",
      "AK 105\n",
      "AK 106\n",
      "AK 107\n",
      "AK 108\n",
      "AK 109\n",
      "AK 110\n",
      "AK 111\n",
      "AK 112\n",
      "AK 113\n",
      "AK 114\n",
      "AK 115\n",
      "AK 116\n",
      "AK 117\n",
      "AK 118\n",
      "AK 119\n",
      "AK 120\n",
      "AK 121\n",
      "AK 122\n",
      "AK 123\n",
      "AK 124\n",
      "AK 125\n",
      "AK 126\n",
      "AK 127\n",
      "AK 128\n",
      "AK 129\n",
      "AK 130\n",
      "AK 131\n",
      "AK 132\n",
      "AK 133\n",
      "AK 134\n",
      "AK 135\n",
      "AK 136\n",
      "AK 137\n",
      "AK 138\n",
      "AK 139\n",
      "AK 140\n",
      "AK 141\n",
      "AK 142\n",
      "142 Didnt work, 55K08S\n",
      "AK 143\n",
      "AK 144\n",
      "AK 145\n",
      "AK 146\n",
      "AK 147\n",
      "AK 148\n",
      "148 Didnt work, 54K01S\n",
      "AK 149\n",
      "AK 150\n",
      "AK 151\n",
      "AK 152\n",
      "AK 153\n",
      "AK 154\n",
      "AK 155\n",
      "AK 156\n",
      "AK 157\n",
      "AK 158\n",
      "AK 159\n",
      "AK 160\n",
      "AK 161\n",
      "AK 162\n",
      "AK 163\n",
      "AK 164\n",
      "AK 165\n",
      "AK 166\n",
      "AK 167\n",
      "AK 168\n",
      "AK 169\n",
      "AK 170\n",
      "AK 171\n",
      "AK 172\n",
      "AK 173\n",
      "AK 174\n",
      "AK 175\n",
      "AK 176\n",
      "AK 177\n",
      "AK 178\n",
      "AK 179\n",
      "AK 180\n",
      "AK 181\n",
      "AK 182\n",
      "AK 183\n",
      "AK 184\n",
      "AK 185\n",
      "AK 186\n",
      "AK 187\n",
      "AK 188\n",
      "AK 189\n",
      "AK 190\n",
      "AK 191\n",
      "AK 192\n",
      "AK 193\n",
      "AK 194\n",
      "AK 195\n",
      "AK 196\n",
      "AK 197\n",
      "AK 198\n",
      "AK 199\n",
      "AK 200\n",
      "AK 201\n",
      "AK 202\n",
      "AK 203\n",
      "AK 204\n",
      "AK 205\n",
      "AK 206\n",
      "AK 207\n",
      "AK 208\n",
      "AK 209\n",
      "AK 210\n",
      "AK 211\n",
      "AK 212\n",
      "AK 213\n",
      "AK 214\n",
      "AK 215\n",
      "AK 216\n",
      "216 Didnt work, nan\n",
      "AK 217\n",
      "AK 218\n",
      "AK 219\n",
      "AK 220\n",
      "AK 221\n",
      "AK 222\n",
      "AK 223\n",
      "AK 224\n",
      "AK 225\n",
      "AK 226\n",
      "AK 227\n",
      "AK 228\n",
      "AK 229\n",
      "AK 230\n",
      "AK 231\n",
      "AK 232\n",
      "AK 233\n",
      "AK 234\n",
      "AK 235\n",
      "AK 236\n",
      "AK 237\n",
      "AK 238\n",
      "AK 239\n",
      "AK 240\n",
      "AK 241\n",
      "AK 242\n",
      "AK 243\n",
      "AK 244\n",
      "AK 245\n",
      "AK 246\n",
      "AK 247\n",
      "AK 248\n",
      "AK 249\n",
      "AK 250\n",
      "AK 251\n",
      "AK 252\n",
      "AK 253\n",
      "AK 254\n",
      "AK 255\n",
      "AK 256\n",
      "AK 257\n",
      "AK 258\n",
      "AK 259\n",
      "AK 260\n",
      "AK 261\n",
      "AK 262\n",
      "AK 263\n",
      "AK 264\n",
      "AK 265\n",
      "AK 266\n"
     ]
    }
   ],
   "source": [
    "states = [\"AK\"] #, \"AZ\", \"CA\", \"CO\", \"ID\", \"MT\", \"NV\", \"NM\", \"OR\", \"SD\", \"UT\", \"WA\", \"WY\"]\n",
    "\n",
    "\n",
    "def find_urls(regex, output=None):\n",
    "    \"\"\"Takes a string of a regex object or string containing html code, extracts all urls found in the article and\n",
    "\n",
    "    Args:\n",
    "        regex (obj)/(str): A String consisting of html code (from a wikipedia article) or a regex object containing a string of html\n",
    "        Output(str)      : (Optional) Of passed, saved all urls to a file with name $output\n",
    "    Writes:\n",
    "        list            : (Optional) Is output variable is passed, list will be written\n",
    "    Returns:\n",
    "        list             : List of the all urls found in article without duplicates\n",
    "    \"\"\"\n",
    "    r = regex\n",
    "    # Determines wether a string or object is passed, creates a list of possible urls\n",
    "    if type(r) == str:\n",
    "\n",
    "        pat = r\"(?<=href\\=\\\").+?(?=\\\"|\\#|\\ t)\"\n",
    "        urls = re.findall(\n",
    "            pat,\n",
    "            r,\n",
    "        )\n",
    "    else:\n",
    "        pat = r\"(?<=href\\=\\\").+?(?=\\\"|\\#|\\ t)\"\n",
    "        urls = re.findall(\n",
    "            pat,\n",
    "            r.text,\n",
    "        )\n",
    "\n",
    "    # Reject urls were used for toubleshooting and locating missed formats\n",
    "    lst = []\n",
    "    reject = []\n",
    "\n",
    "    # For each url, tries to reconstruct links by comparing, then appending to string. Then appenging fixed link to output list. If not reconstructable, added to rejects\n",
    "    for u in urls:\n",
    "\n",
    "        if u[-5:] == \"_hist\":\n",
    "\n",
    "            u = \"https://wcc.sc.egov.usda.gov\" + u\n",
    "            # Partitioning the url is done in two steps due to http: and https: both containing ':'\n",
    "            head, sep, tail = u.partition(\":\")\n",
    "            head2, sep2, tail = tail.partition(\":\")\n",
    "            string = head + sep + head2\n",
    "            lst.append(string)\n",
    "\n",
    "    urls = list(dict.fromkeys(lst))\n",
    "\n",
    "    # Writes links to text file\n",
    "    if output != None:\n",
    "        with open(output, \"w\") as f:\n",
    "            for x in urls:\n",
    "                f.write(x + \"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def scrap_data(save_loc=None):\n",
    "    \"\"\"\n",
    "    Takes a file direcotry string as optional input, then scraps data from USDA SNOTEL measurements and saves \n",
    "    to specified location\n",
    "    \"\"\"\n",
    "    mont_dict = {\n",
    "        \"Dec\": \"-12-\",\n",
    "        \"Jan\": \"-01-\",\n",
    "        \"Feb\": \"-02-\",\n",
    "        \"Mar\": \"-03-\",\n",
    "        \"Apr\": \"-04-\",\n",
    "        \"May\": \"-05-\",\n",
    "        \"Jun\": \"-06-\",\n",
    "        \"Jul\": \"-07-\",\n",
    "        \"Aug\": \"-08-\",\n",
    "        \"Sep\": \"-08-\",\n",
    "    }\n",
    "\n",
    "    # Base url\n",
    "    url = \"https://wcc.sc.egov.usda.gov/nwcc/rgrpt?report=snowmonth_hist&state=\"\n",
    "    for state in states:\n",
    "        r = get(url + state)\n",
    "        urls = find_urls(r)\n",
    "\n",
    "        t = get(urls[0])\n",
    "\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        soup_table = soup.find_all(\"table\")\n",
    "\n",
    "        df = pd.read_html(str(soup_table[1]))[0]\n",
    "        \n",
    "        col_names = [\n",
    "            \"Year\",\n",
    "            \"Jan_date\",\n",
    "            \"Jan_SD\",\n",
    "            \"Jan_SWE\",\n",
    "            \"Feb_date\",\n",
    "            \"Feb_SD\",\n",
    "            \"Feb_SWE\",\n",
    "            \"Mar_date\",\n",
    "            \"Mar_SD\",\n",
    "            \"Mar_SWE\",\n",
    "            \"Apr_date\",\n",
    "            \"Apr_SD\",\n",
    "            \"Apr_SWE\",\n",
    "            \"May_date\",\n",
    "            \"May_SD\",\n",
    "            \"May_SWE\",\n",
    "            \"Jun_date\",\n",
    "            \"Jun_SD\",\n",
    "            \"Jun_SWE\",\n",
    "        ]\n",
    "        super_lst = [[\"Date\", \"StationID\", \"Lat\", \"Long\", \"Elevation\", \"SD\", \"SWE\"]]\n",
    "        for i in range(len(urls)):\n",
    "            if i % 10 == 0:\n",
    "                print(state, i)\n",
    "            t = get(urls[i])\n",
    "\n",
    "            # Using B-Soup to create array of data in html file\n",
    "\n",
    "            soup = BeautifulSoup(t.text, \"html.parser\")\n",
    "            li = soup.prettify().split(\"\\n\")\n",
    "            # The data starts after 62 lines\n",
    "            li = li[62:]\n",
    "\n",
    "            lif = []\n",
    "            for l in li:\n",
    "                lif.append(l.split(\",\"))\n",
    "\n",
    "            # Converting data table to pandas dataframe\n",
    "            m_df = pd.DataFrame(lif)\n",
    "\n",
    "            # Rather ugly regex-code, copy-pase code could be removed.\n",
    "            try:\n",
    "                m_df.columns = col_names\n",
    "\n",
    "                for index, m in m_df.iterrows():\n",
    "                    if m.Year != \"\":\n",
    "                        if 2018 > float(m.Year) > 1980:\n",
    "                            if m.Jan_SD != \"\" and m.Jan_SWE != \"\":\n",
    "                                if float(m.Jan_SD) > 10 and float(m.Jan_SWE) > 0:\n",
    "                                    yr = m.Year\n",
    "\n",
    "                                    # Where no date is suplied, first day in month is assumed\n",
    "                                    if m.Jan_date == \"\":\n",
    "                                        d = \"-01-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.Jan_date[0:3]]\n",
    "                                        d = mnt + m.Jan_date[-2:]\n",
    "                                        # Some december measurements are included in following year\n",
    "                                        if m.Jan_date[0:3] == \"Dec\":\n",
    "                                            yr = int(yr) - 1\n",
    "                                    super_lst.append(\n",
    "                                        [\n",
    "                                            str(yr) + d,\n",
    "                                            df.iloc[i, 2],\n",
    "                                            df.iloc[i, 5],\n",
    "                                            df.iloc[i, 6],\n",
    "                                            df.iloc[i, 4],\n",
    "                                            m.Jan_SD,\n",
    "                                            m.Jan_SWE,\n",
    "                                        ]\n",
    "                                    )\n",
    "                            if m.Feb_SD != \"\" and m.Feb_SWE != \"\":\n",
    "                                if float(m.Feb_SD) > 10 and float(m.Feb_SWE) > 0:\n",
    "                                    if m.Feb_date == \"\":\n",
    "                                        d = \"-02-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.Feb_date[0:3]]\n",
    "                                        d = mnt + m.Feb_date[-2:]\n",
    "\n",
    "                                        super_lst.append(\n",
    "                                            [\n",
    "                                                m.Year + d,\n",
    "                                                df.iloc[i, 2],\n",
    "                                                df.iloc[i, 5],\n",
    "                                                df.iloc[i, 6],\n",
    "                                                df.iloc[i, 4],\n",
    "                                                m.Feb_SD,\n",
    "                                                m.Feb_SWE,\n",
    "                                            ]\n",
    "                                        )\n",
    "                            if m.Mar_SD != \"\" and m.Mar_SWE != \"\":\n",
    "                                if float(m.Mar_SD) > 10 and float(m.Mar_SWE) > 0:\n",
    "                                    if m.Mar_date == \"\":\n",
    "                                        d = \"-03-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.Mar_date[0:3]]\n",
    "                                        d = mnt + m.Mar_date[-2:]\n",
    "\n",
    "                                    super_lst.append(\n",
    "                                        [\n",
    "                                            m.Year + d,\n",
    "                                            df.iloc[i, 2],\n",
    "                                            df.iloc[i, 5],\n",
    "                                            df.iloc[i, 6],\n",
    "                                            df.iloc[i, 4],\n",
    "                                            m.Mar_SD,\n",
    "                                            m.Mar_SWE,\n",
    "                                        ]\n",
    "                                    )\n",
    "                            if m.May_SD != \"\" and m.May_SWE != \"\":\n",
    "                                if float(m.May_SD) > 10 and float(m.May_SWE) > 0:\n",
    "                                    if m.May_date == \"\":\n",
    "                                        d = \"-05-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.May_date[0:3]]\n",
    "                                        d = mnt + m.May_date[-2:]\n",
    "\n",
    "                                    super_lst.append(\n",
    "                                        [\n",
    "                                            m.Year + d,\n",
    "                                            df.iloc[i, 2],\n",
    "                                            df.iloc[i, 5],\n",
    "                                            df.iloc[i, 6],\n",
    "                                            df.iloc[i, 4],\n",
    "                                            m.May_SD,\n",
    "                                            m.May_SWE,\n",
    "                                        ]\n",
    "                                    )\n",
    "                            if m.Apr_SD != \"\" and m.Apr_SWE != \"\":\n",
    "                                if float(m.Apr_SD) > 10 and float(m.Apr_SWE) > 0:\n",
    "                                    if m.Apr_date == \"\":\n",
    "                                        d = \"-04-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.Apr_date[0:3]]\n",
    "                                        d = mnt + m.Apr_date[-2:]\n",
    "\n",
    "                                    super_lst.append(\n",
    "                                        [\n",
    "                                            m.Year + d,\n",
    "                                            df.iloc[i, 2],\n",
    "                                            df.iloc[i, 5],\n",
    "                                            df.iloc[i, 6],\n",
    "                                            df.iloc[i, 4],\n",
    "                                            m.Apr_SD,\n",
    "                                            m.Apr_SWE,\n",
    "                                        ]\n",
    "                                    )\n",
    "                            if m.Jun_SD != \"\" and m.Jun_SWE != \"\":\n",
    "                                if float(m.Jun_SD) > 10 and float(m.Jun_SWE) > 0:\n",
    "\n",
    "                                    if m.Jun_date == \"\":\n",
    "                                        d = \"-06-01\"\n",
    "                                    else:\n",
    "                                        mnt = mont_dict[m.Jun_date[0:3]]\n",
    "                                        d = mnt + m.Jun_date[-2:]\n",
    "                                    super_lst.append(\n",
    "                                        [\n",
    "                                            m.Year + d,\n",
    "                                            df.iloc[i, 2],\n",
    "                                            df.iloc[i, 5],\n",
    "                                            df.iloc[i, 6],\n",
    "                                            df.iloc[i, 4],\n",
    "                                            m.Jun_SD,\n",
    "                                            m.Jun_SWE,\n",
    "                                        ]\n",
    "                                    )\n",
    "            # Print error message if unsuccesful\n",
    "            except:\n",
    "                print(str(i) + \" Didnt work\" + \", \" + str(df.iloc[i, 2]))\n",
    "\n",
    "        # Create seperate files for each state\n",
    "        if save_loc==None:\n",
    "            file = open(\"SNOTEL_\" + state + \".csv\", \"w+\", newline=\"\")\n",
    "        else:\n",
    "            file = open(save_loc + \"SNOTEL_\" + state + \".csv\", \"w+\", newline=\"\")\n",
    "        with file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerows(super_lst)\n",
    "        file.close()\n",
    "\n",
    "    \n",
    "\n",
    "scrap_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date StationID    Lat    Long  Elevation  SD  SWE\n",
      "0      1981-01-28     50M01  61.75 -150.89        160  18  4.0\n",
      "1      1981-02-28     50M01  61.75 -150.89        160  32  6.4\n",
      "2      1981-04-28     50M01  61.75 -150.89        160  16  5.0\n",
      "3      1981-03-31     50M01  61.75 -150.89        160  25  7.0\n",
      "4      1982-01-26     50M01  61.75 -150.89        160  24  6.0\n",
      "...           ...       ...    ...     ...        ...  ..  ...\n",
      "16188  2015-04-02     59O03  63.26 -159.57        100  29  6.7\n",
      "16189  2016-01-30     59O03  63.26 -159.57        100  19  3.7\n",
      "16190  2016-03-02     59O03  63.26 -159.57        100  31  5.8\n",
      "16191  2016-04-01     59O03  63.26 -159.57        100  32  6.9\n",
      "16192  2017-03-30     59O03  63.26 -159.57        100  23  4.2\n",
      "\n",
      "[16193 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"SNOTEL_AK.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_station_data(save_loc = \"\"):\n",
    "\n",
    "    # Loading dataframes for each state\n",
    "    #AZ_df = pd.read_csv(save_loc + \"SNOTEL_AZ.csv\")\n",
    "    AK_df = pd.read_csv(save_loc + \"SNOTEL_AK.csv\")\n",
    "    #CA_df = pd.read_csv(save_loc + \"SNOTEL_CA.csv\")\n",
    "    #CO_df = pd.read_csv(save_loc + \"SNOTEL_CO.csv\")\n",
    "    #ID_df = pd.read_csv(save_loc + \"SNOTEL_ID.csv\")\n",
    "    #MT_df = pd.read_csv(save_loc + \"SNOTEL_MT.csv\")\n",
    "    #NM_df = pd.read_csv(save_loc + \"SNOTEL_NM.csv\")\n",
    "    #NV_df = pd.read_csv(save_loc + \"SNOTEL_NV.csv\")\n",
    "    #OR_df = pd.read_csv(save_loc + \"SNOTEL_OR.csv\")\n",
    "    #SD_df = pd.read_csv(save_loc + \"SNOTEL_SD.csv\")\n",
    "    #UT_df = pd.read_csv(save_loc + \"SNOTEL_UT.csv\")\n",
    "    #WA_df = pd.read_csv(save_loc + \"SNOTEL_WA.csv\")\n",
    "    #WY_df = pd.read_csv(save_loc + \"SNOTEL_WY.csv\")\n",
    "\n",
    "    lst = [\n",
    "        #AZ_df,\n",
    "        AK_df,\n",
    "        #CA_df,\n",
    "        #CO_df,\n",
    "        #ID_df,\n",
    "        #MT_df,\n",
    "        #NM_df,\n",
    "        #NV_df,\n",
    "        #OR_df,\n",
    "        #SD_df,\n",
    "        #UT_df,\n",
    "        #WA_df,\n",
    "        #WY_df,\n",
    "    ]\n",
    "    # Creating timeseries for covered time span\n",
    "    ranger = pd.date_range(start=\"1980.01.01\", end=\"1989.12.31\")\n",
    "\n",
    "    st_lst = []\n",
    "    elev = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    \n",
    "\n",
    "    # Creating a list of station meta-data. (coordinates and elevation)\n",
    "    for df in lst:\n",
    "        for index, row in df.iterrows():\n",
    "            if row.StationID in st_lst:\n",
    "                pass\n",
    "            else:\n",
    "                st_lst.append(row.StationID)\n",
    "                elev.append(row.Elevation)\n",
    "                lat.append(row.Lat)\n",
    "                lon.append(row.Long)\n",
    "\n",
    "    # Saving station data\n",
    "    st_inf = pd.DataFrame(st_lst)\n",
    "    elev_inf = pd.DataFrame(elev)\n",
    "    lat_inf = pd.DataFrame(lat)\n",
    "    lon_inf = pd.DataFrame(lon)\n",
    "\n",
    "    station_df = pd.concat([st_inf, elev_inf, lat_inf, lon_inf], axis=1)\n",
    "    station_df.columns = [\"StationID\", \"Elev\", \"Lat\", \"Lon\"]\n",
    "    station_df.set_index(\"StationID\", inplace=True)\n",
    "\n",
    "    station_df.to_csv(\"USA_Stations.csv\")\n",
    "    \n",
    "    # Creating two matrixes for SD and SWE, dates as columns and stations as rows\n",
    "    SD_df = pd.DataFrame(index=st_lst, columns=ranger)\n",
    "    SWE_df = pd.DataFrame(index=st_lst, columns=ranger)\n",
    "\n",
    "    for df in lst:\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            if pd.to_datetime(row.Date).year <1990:\n",
    "\n",
    "                SWE_df[str(row.Date)][row.StationID] = row.SWE\n",
    "                SD_df[str(row.Date)][row.StationID] = row.SD\n",
    "\n",
    "            \n",
    "\n",
    "    # Converting to cm and saving the matrixes for further proccesing\n",
    "    SWE_df = SWE_df*2.54\n",
    "    SD_df = SD_df*2.54\n",
    "    SWE_df.index.names = ['StationID']\n",
    "    SD_df.index.names = ['StationID']\n",
    "    SWE_df.to_csv(\"USA_SWE.csv\")\n",
    "    SD_df.to_csv(\"USA_SD.csv\")\n",
    "\n",
    "\n",
    "combine_station_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two most important matrixes are collected, they are n*m matixes where n = the number of days between 1980.01.01 and 2018.07.01, and m = the number of stations. Stations only have data where measurements exists, the rest is NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  1980-01-01  1980-01-02  1980-01-03  1980-01-04  1980-01-05  \\\n",
      "0        50M01         NaN         NaN         NaN         NaN         NaN   \n",
      "1       50M01S         NaN         NaN         NaN         NaN         NaN   \n",
      "2       41P07S         NaN         NaN         NaN         NaN         NaN   \n",
      "3       51K05S         NaN         NaN         NaN         NaN         NaN   \n",
      "4       49M22S         NaN         NaN         NaN         NaN         NaN   \n",
      "..         ...         ...         ...         ...         ...         ...   \n",
      "213      50M02         NaN         NaN         NaN         NaN         NaN   \n",
      "214      47Q08         NaN         NaN         NaN         NaN         NaN   \n",
      "215      47Q09         NaN         NaN         NaN         NaN         NaN   \n",
      "216      45M02         NaN         NaN         NaN         NaN         NaN   \n",
      "217      59O03         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "     1980-01-06  1980-01-07  1980-01-08  1980-01-09  ...  2018-06-22  \\\n",
      "0           NaN         NaN         NaN         NaN  ...         NaN   \n",
      "1           NaN         NaN         NaN         NaN  ...         NaN   \n",
      "2           NaN         NaN         NaN         NaN  ...         NaN   \n",
      "3           NaN         NaN         NaN         NaN  ...         NaN   \n",
      "4           NaN         NaN         NaN         NaN  ...         NaN   \n",
      "..          ...         ...         ...         ...  ...         ...   \n",
      "213         NaN         NaN         NaN         NaN  ...         NaN   \n",
      "214         NaN         NaN         NaN         NaN  ...         NaN   \n",
      "215         NaN         NaN         NaN         NaN  ...         NaN   \n",
      "216         NaN         NaN         NaN         NaN  ...         NaN   \n",
      "217         NaN         NaN         NaN         NaN  ...         NaN   \n",
      "\n",
      "     2018-06-23  2018-06-24  2018-06-25  2018-06-26  2018-06-27  2018-06-28  \\\n",
      "0           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "213         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "214         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "215         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "216         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "217         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "     2018-06-29  2018-06-30  2018-07-01  \n",
      "0           NaN         NaN         NaN  \n",
      "1           NaN         NaN         NaN  \n",
      "2           NaN         NaN         NaN  \n",
      "3           NaN         NaN         NaN  \n",
      "4           NaN         NaN         NaN  \n",
      "..          ...         ...         ...  \n",
      "213         NaN         NaN         NaN  \n",
      "214         NaN         NaN         NaN  \n",
      "215         NaN         NaN         NaN  \n",
      "216         NaN         NaN         NaN  \n",
      "217         NaN         NaN         NaN  \n",
      "\n",
      "[218 rows x 14063 columns]\n"
     ]
    }
   ],
   "source": [
    "SD = pd.read_csv(\"USA_SD.csv\")\n",
    "print(SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating the features that will predict the SWE in the models. This is done using ERA5-Land data and snowclass. The script bellow is an example of a get request sent to the ECMWF API to get data. The waiting times are long and the recieved files are large. Id order to get all a total of 8 requests needed to be sent, one for each decade and one for each parameter (Hourly temperature and precipitation)\n",
    "The ERA5-Land elevation can be derived from the geopotential found at: https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation#ERA5Land:datadocumentation-parameterlistingParameterlistings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_era5():\n",
    "    import cdsapi\n",
    "\n",
    "    \"\"\"\n",
    "    Retreiving ERA5 data requires a key and precise parameters. This is only meant to serve \n",
    "    as an example of how a request is supposed to look like. For me, era5 was only able\n",
    "    to give 10 years of data at a time. Precipitation data and temperature data are optained seperatly.\n",
    "    Read https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=form for \n",
    "    further information on era5-land data download and key registration,\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-land\",\n",
    "        {\n",
    "            \"format\": \"netcdf\",\n",
    "            \"variable\": \"total_precipitation\",\n",
    "            \"year\": [\n",
    "                \"1980\",\n",
    "                \"1981\",\n",
    "                \"1982\",\n",
    "                \"1983\",\n",
    "                \"1984\",\n",
    "                \"1985\",\n",
    "                \"1986\",\n",
    "                \"1987\",\n",
    "                \"1988\",\n",
    "                \"1989\",\n",
    "            ],\n",
    "            \"month\": [\n",
    "                \"01\",\n",
    "                \"02\",\n",
    "                \"03\",\n",
    "                \"04\",\n",
    "                \"05\",\n",
    "                \"06\",\n",
    "                \"07\",\n",
    "                \"08\",\n",
    "                \"09\",\n",
    "                \"10\",\n",
    "                \"11\",\n",
    "                \"12\",\n",
    "            ],\n",
    "            \"day\": [\n",
    "                \"01\",\n",
    "                \"02\",\n",
    "                \"03\",\n",
    "                \"04\",\n",
    "                \"05\",\n",
    "                \"06\",\n",
    "                \"07\",\n",
    "                \"08\",\n",
    "                \"09\",\n",
    "                \"10\",\n",
    "                \"11\",\n",
    "                \"12\",\n",
    "                \"13\",\n",
    "                \"14\",\n",
    "                \"15\",\n",
    "                \"16\",\n",
    "                \"17\",\n",
    "                \"18\",\n",
    "                \"19\",\n",
    "                \"20\",\n",
    "                \"21\",\n",
    "                \"22\",\n",
    "                \"23\",\n",
    "                \"24\",\n",
    "                \"25\",\n",
    "                \"26\",\n",
    "                \"27\",\n",
    "                \"28\",\n",
    "                \"29\",\n",
    "                \"30\",\n",
    "                \"31\",\n",
    "            ],\n",
    "            \"time\": [\n",
    "                \"00:00\",\n",
    "                \"01:00\",\n",
    "                \"02:00\",\n",
    "                \"03:00\",\n",
    "                \"04:00\",\n",
    "                \"05:00\",\n",
    "                \"06:00\",\n",
    "                \"07:00\",\n",
    "                \"08:00\",\n",
    "                \"09:00\",\n",
    "                \"10:00\",\n",
    "                \"11:00\",\n",
    "                \"12:00\",\n",
    "                \"13:00\",\n",
    "                \"14:00\",\n",
    "                \"15:00\",\n",
    "                \"16:00\",\n",
    "                \"17:00\",\n",
    "                \"18:00\",\n",
    "                \"19:00\",\n",
    "                \"20:00\",\n",
    "                \"21:00\",\n",
    "                \"22:00\",\n",
    "                \"23:00\",\n",
    "            ],\n",
    "            \"area\": [\n",
    "                49,\n",
    "                -124,\n",
    "                32,\n",
    "                -103,\n",
    "            ],\n",
    "        },\n",
    "        \"download_total_precip_1980-1989.nc\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cor(tude, x):\n",
    "    cnt = 0\n",
    "    for d in tude:\n",
    "        if d.values == round(x * 10) / 10:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def era5_proccessing(stations, temp = True):\n",
    "    \"\"\"Processing of era5-land dataset, assuming 10 year intervals of era5-land data in netcdf format.\n",
    "    For the script to work, ecmwflibs needs to be installed through 'pip install ecmwflibs'\n",
    "    \"\"\"\n",
    "        \n",
    "    if temp == True:\n",
    "        df = xr.open_dataset(\"download_alaska_1980-1989.nc\", engine=\"netcdf4\")\n",
    "        saveloc = \"Data/temp/1980/\"\n",
    "        # Printing xarray metadata\n",
    "        for v in df:\n",
    "            print(\"{}, {}, {}\".format(v, df[v].attrs[\"long_name\"], df[v].attrs[\"units\"]))\n",
    "\n",
    "        # Finding start and stop time\n",
    "        t0 = df.t2m[0, 0, 0].time.values\n",
    "        t1 = df.t2m[-1, 0, 0].time.values\n",
    "        # Creating timeseries of dates\n",
    "        dates = pd.date_range(t0, t1, freq=\"d\")\n",
    "        \n",
    "        for index, row in stations.iterrows():\n",
    "            \n",
    "            # Finding temperature using coordinates as index in xarray-df\n",
    "            temp_df = df.t2m.sel(\n",
    "                latitude=row.Lat, longitude=row.Lon, method=\"nearest\"\n",
    "            ).to_dataset()\n",
    "            \n",
    "            # Creating array of temperatures, reshaping so each row/col contains one list of 24 values\n",
    "            # These values are hourly temperatures\n",
    "            arr = temp_df.t2m.values\n",
    "            arr = arr.reshape(-1, 24)\n",
    "\n",
    "            # Calculating minimum and maximum temperature for each day, and storing separately\n",
    "            min_ = np.min(arr, 1)\n",
    "            max_ = np.max(arr, 1)\n",
    "\n",
    "            # For each station, a file is created containing the date, min and max temperature.\n",
    "            sdf = pd.DataFrame([dates, min_, max_]).T\n",
    "            sdf.columns = [\"time\", \"tmin\", \"tmax\"]\n",
    "\n",
    "            sdf.to_csv(saveloc + row.StationID + \"_temp.csv\", index=False)\n",
    "            # These will have to be combined later.\n",
    "    else:\n",
    "        \n",
    "        df = xr.open_dataset(\"download_alaska_1980-1989_pre.nc\", engine=\"netcdf4\")\n",
    "        saveloc = \"Data/tp/1980/\"\n",
    "        # Printing xarray metadata\n",
    "        for v in df:\n",
    "            print(\"{}, {}, {}\".format(v, df[v].attrs[\"long_name\"], df[v].attrs[\"units\"]))\n",
    "\n",
    "        # Finding start and stop time\n",
    "        t0 = df.tp[0, 0, 0].time.values\n",
    "        t1 = df.tp[-1, 0, 0].time.values\n",
    "        # Creating timeseries of dates\n",
    "        dates = pd.date_range(t0, t1, freq=\"d\")\n",
    "\n",
    "        for index, row in stations.iterrows():\n",
    "            #Similar to previous code, but only precipitation sum is stored\n",
    "\n",
    "            a = df.tp.sel(latitude=row.Lat, longitude=row.Lon, method=\"nearest\").to_dataset()\n",
    "\n",
    "            arr = a.tp.values\n",
    "            arr = arr.reshape(-1, 24)\n",
    "            sum_ = np.sum(arr, 1)\n",
    "\n",
    "            sdf = pd.DataFrame([dates, sum_]).T\n",
    "            sdf.columns = [\"time\", \"tp\"]\n",
    "            sdf.to_csv(saveloc + row.StationID + \"_tp.csv\", index=False)\n",
    "\n",
    "\n",
    "def elevation_data(stations):\n",
    "    #Elevation of the ERA5-Land gridcell is found through the geopotential\n",
    "    #Elevation from SNOTEL is given in feet and needs to be converted to meter\n",
    "\n",
    "    elev_xar = xr.open_dataset(\"geo_1279l4_0.1x0.1.grib2_v4_unpack.nc\", engine=\"netcdf4\")\n",
    "    # Printing xarray metadata\n",
    "    for v in elev_xar:\n",
    "        print(\"{}, {}, {}\".format(v, elev_xar[v].attrs[\"long_name\"], elev_xar[v].attrs[\"units\"]))\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"StationID\", \"Elev\", \"Era5_elev\"])\n",
    "    for index, row in stations.iterrows():\n",
    "        # Finding temperature using coordinates as index in xarray-df\n",
    "        temp_df = elev_xar.z.sel(\n",
    "            latitude=row.Lat, longitude=row.Lon+360, method=\"nearest\"\n",
    "        ).to_dataset()\n",
    "\n",
    "        #Converting to meters\n",
    "        \n",
    "        df.loc[len(df)]=[row.StationID,temp_df.z.values[0]/9.80665,row.Elev*0.3048]       \n",
    "        df.to_csv(\"USA_Elevations.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z, Geopotential, m**2 s**-2\n"
     ]
    }
   ],
   "source": [
    "stations = pd.read_csv(\"USA_Stations.csv\")\n",
    "elevation_data(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2m, 2 metre temperature, K\n"
     ]
    }
   ],
   "source": [
    "era5_proccessing(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp, Total precipitation, m\n"
     ]
    }
   ],
   "source": [
    "era5_proccessing(stations, temp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If several ERA5 timeseries are used, they need to be combined into one precipitation and one temperature file for each station. The following function is an example of how this can be done using the file structure used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ERA_data(variable=\"tp\"):\n",
    "    stations = pd.read_csv(\"USA_stations.csv\")\n",
    "\n",
    "    for index, row in stations.iterrows():\n",
    "        df_80 = pd.read_csv(\"Data/\"+variable+\"/1980/\" + row.StationID + \".csv\")\n",
    "        df_90 = pd.read_csv(\"Data/\"+variable+\"/1990/\" + row.StationID + \".csv\")\n",
    "        df_00 = pd.read_csv(\"Data/\"+variable+\"/2000/\" + row.StationID + \".csv\")\n",
    "        df_10 = pd.read_csv(\"Data/\"+variable+\"/2010/\" + row.StationID + \".csv\")\n",
    "        \n",
    "        df = df_80.append(df_90.append(df_00.append(df_10)))\n",
    "        df.to_csv(\"Data/tp/combined\" + row.StationID + \".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERA_matrix_creator():\n",
    "\n",
    "    ranger = pd.date_range(start=\"1980.01.01\", end=\"1989.12.31\")\n",
    "    stations = pd.read_csv(\"USA_stations.csv\")\n",
    "    df_min = pd.DataFrame(columns=ranger)\n",
    "    df_max = pd.DataFrame(columns=ranger)\n",
    "    df_tp = pd.DataFrame(columns=ranger)\n",
    "\n",
    "\n",
    "    for index, row in stations.iterrows():\n",
    "        print(row.StationID, index)\n",
    "\n",
    "        #Transposing the matrix to get the met-data as rows.\n",
    "        mat = pd.read_csv(\"Data/temp/1980/\" + row.StationID + \"_temp.csv\").T\n",
    "\n",
    "        df_min.loc[row.StationID] = mat.iloc[1, :].to_numpy()\n",
    "        df_max.loc[row.StationID] = mat.iloc[2, :].to_numpy()\n",
    "\n",
    "        mat = pd.read_csv(\"Data/tp/1980/\" + row.StationID + \"_tp.csv\").T\n",
    "\n",
    "        df_tp.loc[row.StationID] = mat.iloc[1, :].to_numpy()\n",
    "\n",
    "    df_min.to_csv(\"USA_data_tmin.csv\")\n",
    "    df_max.to_csv(\"USA_data_tmax.csv\")\n",
    "    df_tp.to_csv(\"USA_data_tp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50M01 0\n",
      "50M01S 1\n",
      "41P07S 2\n",
      "51K05S 3\n",
      "49M22S 4\n",
      "59M02 5\n",
      "49M25 6\n",
      "49M05 7\n",
      "49M01 8\n",
      "49M02 9\n",
      "49M03 10\n",
      "49M04 11\n",
      "50LL24 12\n",
      "50LL21 13\n",
      "49L02 14\n",
      "51R01S 15\n",
      "49M28 16\n",
      "49N07 17\n",
      "48P01 18\n",
      "50R03 19\n",
      "47Q07 20\n",
      "41P03 21\n",
      "51K01 22\n",
      "59O04 23\n",
      "47Q03 24\n",
      "45P02 25\n",
      "47Q04 26\n",
      "42Q02 27\n",
      "51N01 28\n",
      "41P02 29\n",
      "42N01 30\n",
      "42N01S 31\n",
      "44N01 32\n",
      "43M01 33\n",
      "44Q05 34\n",
      "46N01 35\n",
      "47Q01 36\n",
      "57Q02 37\n",
      "43Q02 38\n",
      "50S01 39\n",
      "50S01S 40\n",
      "46P04 41\n",
      "57Q03 42\n",
      "51M03 43\n",
      "49L10S 44\n",
      "45N04 45\n",
      "43P01 46\n",
      "43P02 47\n",
      "34J02 48\n",
      "46N05 49\n",
      "51KK17 50\n",
      "44M01 51\n",
      "55P01 52\n",
      "51K03 53\n",
      "50N06 54\n",
      "49S01 55\n",
      "49O02 56\n",
      "50N03 57\n",
      "49O01 58\n",
      "34J03 59\n",
      "51K06 60\n",
      "51R06 61\n",
      "49L18 62\n",
      "49L18S 63\n",
      "46Q06 64\n",
      "45O01 65\n",
      "45O01S 66\n",
      "34J04 67\n",
      "49M10 68\n",
      "54L02 69\n",
      "48N02 70\n",
      "45O05 71\n",
      "45R01 72\n",
      "47Q10 73\n",
      "50KK09 74\n",
      "46P02 75\n",
      "56P01 76\n",
      "44O02 77\n",
      "49L09S 78\n",
      "45O04S 79\n",
      "51M02 80\n",
      "43S03 81\n",
      "49L14S 82\n",
      "45N01 83\n",
      "49Q02 84\n",
      "47N02 85\n",
      "54Q02 86\n",
      "49M26 87\n",
      "49M26S 88\n",
      "49M08S 89\n",
      "32H04 90\n",
      "41PP8S 91\n",
      "41N01 92\n",
      "50L01 93\n",
      "57Q01 94\n",
      "51R02 95\n",
      "50O01 96\n",
      "52R02 97\n",
      "52R04 98\n",
      "62S01S 99\n",
      "50L02S 100\n",
      "49L03 101\n",
      "45M05 102\n",
      "50M04 103\n",
      "31G09 104\n",
      "46N02 105\n",
      "52O01 106\n",
      "52R01 107\n",
      "46Q05 108\n",
      "46Q02 109\n",
      "46Q02S 110\n",
      "53P01 111\n",
      "46M06 112\n",
      "49M24 113\n",
      "51M07 114\n",
      "44M03 115\n",
      "33J01S 116\n",
      "41P05 117\n",
      "43N02 118\n",
      "45M03 119\n",
      "46R02 120\n",
      "54P01 121\n",
      "42M01S 122\n",
      "55N01 123\n",
      "51K14S 124\n",
      "43N01 125\n",
      "58O02 126\n",
      "51R03 127\n",
      "31G05 128\n",
      "47O01 129\n",
      "46N03 130\n",
      "45Q02 131\n",
      "45Q02S 132\n",
      "35K02 133\n",
      "49L04 134\n",
      "48M04S 135\n",
      "35K05 136\n",
      "49L01S 137\n",
      "45L01S 138\n",
      "42O01 139\n",
      "46Q01 140\n",
      "46Q01S 141\n",
      "46P01 142\n",
      "46P01S 143\n",
      "58P02 144\n",
      "52R03 145\n",
      "41M01 146\n",
      "50N01 147\n",
      "50K06 148\n",
      "41N02 149\n",
      "63P02 150\n",
      "49L05 151\n",
      "45O11 152\n",
      "32H01 153\n",
      "32H03 154\n",
      "58P01 155\n",
      "54L01 156\n",
      "51K15S 157\n",
      "48L04 158\n",
      "45Q03 159\n",
      "52N01 160\n",
      "33H05 161\n",
      "50N04 162\n",
      "49L07 163\n",
      "48O03 164\n",
      "63P01 165\n",
      "45N02 166\n",
      "49Q01 167\n",
      "45P01 168\n",
      "47M02 169\n",
      "51M01 170\n",
      "49L11 171\n",
      "49M11 172\n",
      "33J03 173\n",
      "47N01 174\n",
      "57P01 175\n",
      "46M01 176\n",
      "45Q04 177\n",
      "41Q02 178\n",
      "46M04 179\n",
      "49L19S 180\n",
      "50N07S 181\n",
      "49S03 182\n",
      "50N02 183\n",
      "45N03 184\n",
      "44M02 185\n",
      "53L01 186\n",
      "53L01S 187\n",
      "45P03 188\n",
      "45P03S 189\n",
      "50R02 190\n",
      "43P03 191\n",
      "43O01 192\n",
      "50N05 193\n",
      "50N05S 194\n",
      "46N04 195\n",
      "56Q01 196\n",
      "45M04 197\n",
      "49L13S 198\n",
      "46M03 199\n",
      "47N03 200\n",
      "44Q01 201\n",
      "44Q07S 202\n",
      "56O01 203\n",
      "47N07 204\n",
      "47N06 205\n",
      "31G10 206\n",
      "45M07S 207\n",
      "53L02 208\n",
      "46M02 209\n",
      "46R03 210\n",
      "57O02 211\n",
      "35K04 212\n",
      "50M02 213\n",
      "47Q08 214\n",
      "47Q09 215\n",
      "45M02 216\n",
      "59O03 217\n"
     ]
    }
   ],
   "source": [
    "ERA_matrix_creator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snowclasses are obtained from Sturm et al. (2021): https://journals.ametsoc.org/view/journals/hydr/22/11/JHM-D-21-0070.1.xml, with the data available at: https://nsidc.org/data/nsidc-0768/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_snowclass(stations):\n",
    "    df = xr.open_dataset(\"SnowClass_GL_05km_2.50arcmin_2021_v01.0.nc\", engine=\"netcdf4\")\n",
    "    stations[\"Class\"] = \"\"\n",
    "\n",
    "    for i in range(stations.shape[0]):\n",
    "        stations[\"Class\"][i] = df.SnowClass.sel(\n",
    "            lat=stations.iloc[i, 2],\n",
    "            lon=stations.iloc[i, 3],\n",
    "            method=\"nearest\",\n",
    "        ).values\n",
    "        \n",
    "    \n",
    "    #Dictionary of snowclass codes.\n",
    "    #Maritime is used twice since coastal stations were mislabeled as 8.0 (Ocean)\n",
    "    g = {\n",
    "        1.0: 1, #\"Tundra\",\n",
    "        2.0: 2, #\"Boreal Forest\",\n",
    "        3.0: 3, #\"Maritime\",\n",
    "        4.0: 4, #\"Ephemoral\",\n",
    "        5.0: 5, #\"Praire\",\n",
    "        6.0: 6, #\"Mountane Forest\",\n",
    "        7.0: 3, #\"Ice\",\n",
    "        8.0: 3, #\"Ocean\",\n",
    "        9.0: 3  #\"Fill\",\n",
    "    }\n",
    "    \n",
    "    stations[\"Class\"] = stations[\"Class\"].replace(g)\n",
    "\n",
    "    return stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    StationID  Elev    Lat     Lon  Class\n",
      "0       50M01   160  61.75 -150.89      6\n",
      "1      50M01S   160  61.75 -150.89      6\n",
      "2      41P07S  1050  64.79 -141.23      2\n",
      "3      51K05S  1653  59.86 -151.32      6\n",
      "4      49M22S  2080  61.11 -149.67      2\n",
      "..        ...   ...    ...     ...    ...\n",
      "213     50M02   200  61.75 -150.05      6\n",
      "214     47Q08  1900  65.57 -147.42      2\n",
      "215     47Q09  1200  65.55 -147.67      1\n",
      "216     45M02  2100  61.18 -145.69      1\n",
      "217     59O03   100  63.26 -159.57      2\n",
      "\n",
      "[218 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "Stations = pd.read_csv(\"USA_Stations.csv\")\n",
    "Stations = Stations[[\"StationID\", \"Elev\", \"Lat\", \"Lon\"]]\n",
    "Stations = add_snowclass(Stations)\n",
    "print(Stations)\n",
    "\n",
    "Stations.to_csv(\"USA_Stations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both SD/SWE data and ERA5 data for precipitation, min/max temperature, and elevation data is gathered it is time to create the final dataset that the models will use for training/prediction. The code used for this was developed by Ntokas et al. in their 2021 paper \"Investigating ANN architectures and training to estimate snow water equivalent from snow depth\" (https://hess.copernicus.org/articles/25/3017/2021/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code developed by ntokas et al., edited by me. Fore more information see https://hess.copernicus.org/articles/25/3017/2021/\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_met_data(Lat, Long, Station, year, par_dir):\n",
    "    # get lat and long of each Nivo Station rounded on one digit\n",
    "    Lat_Meteo = np.round(Lat, 1)\n",
    "    Long_Meteo = np.round(Long, 1)\n",
    "    \n",
    "    # set up dates for meteo data\n",
    "    start = datetime(year, 1, 1)\n",
    "    if year == 2019:\n",
    "        end = datetime(year, 4, 30)\n",
    "    else:\n",
    "        end = datetime(year, 12, 31)\n",
    "    dates_Meteo = pd.date_range(start, end)\n",
    "    \n",
    "    # get precip, tmin, tmax for each Nivo Station for each day \n",
    "    num_station = len(Lat)\n",
    "    num_dates = len(dates_Meteo)\n",
    "    total_precip = np.empty((num_station, num_dates))\n",
    "    total_precip[:] = np.nan\n",
    "    tmax = np.empty((num_station, num_dates))\n",
    "    tmax[:] = np.nan\n",
    "    tmin = np.empty((num_station, num_dates))\n",
    "    tmin[:] = np.nan\n",
    "    \n",
    "    if year == 2019:\n",
    "        months = [1, 2, 3, 4]\n",
    "    else:\n",
    "        months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    for month in months:\n",
    "        data_tp = pickle.load(open(par_dir + \"/00_data/saved_variables/ERA5/ERA5_data_tp_{0:02d}_{1:04d}\".format(month, year), \"rb\"))  \n",
    "        data_tmintmax = pickle.load(open(par_dir + \"/00_data/saved_variables/ERA5/ERA5_data_tmaxtmin_{0:02d}_{1:04d}\".format(month, year), \"rb\"))  \n",
    "        date_start = datetime(year, month, 1)\n",
    "        num_days = len(data_tp['Date'])\n",
    "        idx_date, = np.where(dates_Meteo == np.datetime64(date_start))\n",
    "        data_tp['Lat'] = np.round(data_tp['Lat'], decimals=1)\n",
    "        data_tp['Long'] = np.round(data_tp['Long'], decimals=1)\n",
    "        for i in range(num_station): \n",
    "            idx_lat = np.where(data_tp['Lat'] == Lat_Meteo[i])\n",
    "            idx_lon = np.where(data_tp['Long'] == Long_Meteo[i])\n",
    "            total_precip[i, int(idx_date):int(idx_date+num_days)] = np.squeeze(data_tp['TotalPrecip(mm)'][: ,idx_lat, idx_lon])\n",
    "            tmin[i, int(idx_date):int(idx_date+num_days)] = np.squeeze(data_tmintmax['Min_t2m(C)'][: ,idx_lat, idx_lon])\n",
    "            tmax[i, int(idx_date):int(idx_date+num_days)] = np.squeeze(data_tmintmax['Max_t2m(C)'][: ,idx_lat, idx_lon])\n",
    "            \n",
    "    dates_Meteo = dates_Meteo.values.astype('datetime64[D]')\n",
    "    \n",
    "    return total_precip, tmin, tmax, dates_Meteo\n",
    "\n",
    "def frost_defrost(tmin_mod, tmax_mod, dates, nb_dates):\n",
    "    Frost_NoDefrost = np.nonzero(np.logical_and(tmin_mod<=-1, tmax_mod<1).values)\n",
    "    Frost_Defrost = np.nonzero(np.logical_and(tmin_mod<=-1, tmax_mod>=1).values)\n",
    "    NoFrost_NoDefrost = np.nonzero(np.logical_and(tmin_mod>-1, tmax_mod<1).values)\n",
    "    NoFrost_Defrost = np.nonzero(np.logical_and(tmin_mod>-1, tmax_mod>=1).values)\n",
    "\n",
    "    episode = np.empty(nb_dates)\n",
    "    episode[Frost_NoDefrost] = 1\n",
    "    episode[Frost_Defrost] = 2\n",
    "    episode[NoFrost_NoDefrost] = 3\n",
    "    episode[NoFrost_Defrost] = 4\n",
    "    count = 0 \n",
    "    frost = 0\n",
    "    result = np.zeros(nb_dates)\n",
    "    for i in range(nb_dates):\n",
    "        date = dates[i]\n",
    "        if (date.month == 9) and (date.day == 1):\n",
    "            count = 0\n",
    "        if episode[i] == 1: \n",
    "            frost = 1\n",
    "        elif episode[i] == 2: \n",
    "            count += 1\n",
    "            frost = 0\n",
    "        elif episode[i] == 3: \n",
    "            count = count \n",
    "            frost = frost\n",
    "        elif episode[i] == 4: \n",
    "            if frost == 1:\n",
    "                count += 1\n",
    "                frost = 0\n",
    "        result[i] = count \n",
    "    result_df = pd.DataFrame(result, index=dates)\n",
    "    return result_df\n",
    "\n",
    "def num_without_snow(tmax_mod, total_precip_mod, dates, nb_dates):\n",
    "    result = np.zeros(nb_dates)\n",
    "    logic = ~np.logical_and(tmax_mod<=0, total_precip_mod>=3)\n",
    "    count = 0\n",
    "    for i in range(nb_dates):\n",
    "        date = dates[i]\n",
    "        if (date.month == 9) and (date.day == 1):\n",
    "            count = 0\n",
    "        if logic[i] == 1:\n",
    "            count += 1\n",
    "        result[i] = count\n",
    "    result_df = pd.DataFrame(result, index=dates)   \n",
    "    return result_df\n",
    "\n",
    "def pos_degrees(tmid, dates, nb_dates):\n",
    "    result = np.zeros(nb_dates)\n",
    "    tmid_mod = np.where(tmid > 0, tmid, 0)\n",
    "    count = 0\n",
    "    for i in range(nb_dates):\n",
    "        date = dates[i]\n",
    "        if (date.month == 9) and (date.day == 1):\n",
    "            count = 0\n",
    "        count = count + tmid_mod[i]\n",
    "        result[i] = count\n",
    "    result_df = pd.DataFrame(result, index=dates)   \n",
    "    return result_df\n",
    "\n",
    "def num_layer(delta, th, total_precip_solid, dates, nb_dates):\n",
    "    result = np.zeros(nb_dates)\n",
    "    count = 0\n",
    "    new_layer = 0\n",
    "    cumul = np.sum(total_precip_solid[:delta])\n",
    "    if cumul > th: \n",
    "        count += 1\n",
    "        new_layer = 1\n",
    "        result[:delta] = 1\n",
    "    for i in range(delta, nb_dates):\n",
    "        date = dates[i]\n",
    "        if (date.month == 9) and (date.day == 1):\n",
    "            count = 0\n",
    "        if (new_layer == 0) and (total_precip_solid[i] > 0):\n",
    "            count += 1\n",
    "            new_layer = 1\n",
    "        cumul = np.sum(total_precip_solid[(i-delta) + 1 :i + 1])\n",
    "        if (new_layer == 1) and (cumul <= th):\n",
    "            new_layer = 0\n",
    "        result[i] = count \n",
    "    result_df = pd.DataFrame(result, index=dates)   \n",
    "    return result_df\n",
    "\n",
    "def age_snow_cover(d, tmid, total_precip_solid):\n",
    "    dt = pd.to_datetime(d)\n",
    "    month = dt.month\n",
    "    year = dt.year\n",
    "    if month < 9:\n",
    "        year = year - 1\n",
    "    else:\n",
    "        year = year\n",
    "    sep_1st = date(year, 9, 1)\n",
    "    sep_1st = str(year) + \"-09-01\"\n",
    "    tmid_mod = tmid.loc[sep_1st:d]\n",
    "    nb = len(tmid_mod)\n",
    "    total_precip_mod_solid = total_precip_solid[sep_1st:d]\n",
    "    cuml = np.sum(total_precip_mod_solid)\n",
    "    age = np.arange(nb, 0, -1)\n",
    "    age_acc = np.sum(age * total_precip_mod_solid)\n",
    "    result = age_acc / cuml\n",
    "    return result, total_precip_mod_solid, cuml, nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code developed by ntokas et al., edited by me. Fore more information see https://hess.copernicus.org/articles/25/3017/2021/\n",
    "\"\"\"\n",
    "\n",
    "def feature_calculations(SWE, SD, total_precip, tmin, tmax, Stations, savefile, elevations):\n",
    "    # load Canadian historical sonw survey (CHSS): dictionary with Station ID, longitude, latitude, elevation,\n",
    "    #                                              2 dim matrix (Station ID x dates) for SWE, SD, density\n",
    "    \n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load associated meteorological data\n",
    "    total_precip = pd.read_csv(\"USA_data_tp.csv\", index_col=False)\n",
    "    total_precip = total_precip * 1000\n",
    "\n",
    "\n",
    "    print(\"Snow survey data and meteorological data loaded\")\n",
    "\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "\n",
    "    # correction of temperature because of different elevation of station and grid\n",
    "    # load elevation of grid points\n",
    "    # round Longitude and Latitude to the first decimal to fit them together (ERA5 lowest resolution is 0.1x0.1)\n",
    "\n",
    "    Lat_station = Stations[\"Lat\"].round(decimals=1)\n",
    "    Long_station = Stations[\"Lon\"].round(decimals=1)\n",
    "    # apply temperature correction\n",
    "    for i in Lat_station.index:\n",
    "        elev_grid = elevations[\"Era5_elev\"].loc[i]\n",
    "        elev_st = elevations[\"Elev\"].loc[i]\n",
    "        tmin.iloc[i, :] = tmin.iloc[i, :] + ((elev_st - elev_grid) / 1000 * (-6)) - 273.15\n",
    "        tmax.iloc[i, :] = tmax.iloc[i, :] + ((elev_st - elev_grid) / 1000 * (-6)) - 273.15\n",
    "\n",
    "\n",
    "    # load function to calculate input varibles from meteorological data\n",
    "    import _input_calc\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # create dataset for input;\n",
    "    # one line represents one records of SD and the associated input varibles, which will be calculated in the following\n",
    "    df_dataset = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"StationID\",\n",
    "            \"Date\",\n",
    "            \"SD\",\n",
    "            \"SWE\",\n",
    "            \"Elev\",\n",
    "            \"Lat\",\n",
    "            \"Long\",\n",
    "            \"DOY\",\n",
    "            \"days without snow\",\n",
    "            \"number frost-defrost\",\n",
    "            \"accum pos degrees\",\n",
    "            \"average age SC\",\n",
    "            \"number layer\",\n",
    "            \"accum solid precip\",\n",
    "            \"accum solid precip in last 10 days\",\n",
    "            \"total precip last 10 days\",\n",
    "            \"average temp last 6 days\",\n",
    "            \"snowclass\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    start_year = pd.to_datetime(tmin.columns[0]).year\n",
    "    end_year = pd.to_datetime(tmin.columns[-1]).year\n",
    "    range_year = np.arange(start_year + 1, end_year + 1)\n",
    "    dates_Meteo = pd.to_datetime(tmin.columns)\n",
    "    nb_dates = len(dates_Meteo)\n",
    "\n",
    "\n",
    "    # count to print how may stations are left\n",
    "    count = len(Stations.index)\n",
    "    line = 0\n",
    "    for St in Stations.index:\n",
    "        \n",
    "\n",
    "        count = count - 1\n",
    "        if count % 25 == 0:\n",
    "            print(datetime.now())\n",
    "            print(\"{} left till loop over stations done\".format(count))\n",
    "            print(\"Data points created: \", line)\n",
    "        # calculate average temeperature\n",
    "        tmid = (tmin.iloc[St, :] + tmax.iloc[St, :]) / 2\n",
    "\n",
    "        # logistic regression to separate precipitation into silod and liquid parts\n",
    "        # tested over the northern hemnisphere by\n",
    "        # ('Spatial variation of the rainsnow temperature threshold across the Northern Hemisphere' Jennings et al. 2018)\n",
    "        probab_snow = 1 / (1 + np.exp(-1.54 + 1.24 * tmid))\n",
    "        total_precip_solid = total_precip.iloc[St, :] * probab_snow\n",
    "\n",
    "        # generate frost-defrost timeline for concerning station;\n",
    "        # -1C for the maximal temp and 1C for the minimal temperature are the threshold for freezing and thawing, respectively\n",
    "        frost_defrost_vect = _input_calc.frost_defrost(\n",
    "            tmin.iloc[St, :], tmax.iloc[St, :], dates_Meteo, nb_dates\n",
    "        )\n",
    "        # print(tmin.iloc[St, 0])\n",
    "        # generate timeline of number of days without snow for concerning station\n",
    "        nb_days_without_snow_vect = _input_calc.num_without_snow(\n",
    "            tmax.iloc[St, :], total_precip.iloc[St, 1:], dates_Meteo, nb_dates\n",
    "        )\n",
    "        # generate timeline of accumulated posiitve degrees since beginning of winter (1. of September)\n",
    "        pos_degree_vect = _input_calc.pos_degrees(tmid, dates_Meteo, nb_dates)\n",
    "        # generate timeline of number of layers;\n",
    "        # a new layer is considered to be created if there is a 3-days gap\n",
    "        # with less than 10mm of (accumulated over these 3 days) solid precipitation\n",
    "        num_layer_vec = _input_calc.num_layer(\n",
    "            3, 10, total_precip_solid, dates_Meteo, nb_dates\n",
    "        )\n",
    "\n",
    "        # loop over the dates for the concerning station\n",
    "        for dat in SWE.columns:\n",
    "            if dat != \"StationID\":\n",
    "\n",
    "                if ~np.isnan(SWE.loc[St, dat]):\n",
    "                    # preparation\n",
    "\n",
    "                    ndRow = np.empty((1, len(df_dataset.columns)))\n",
    "                    ndRow[:] = np.nan\n",
    "                    row = pd.DataFrame(ndRow, columns=df_dataset.columns)\n",
    "                    df_dataset = df_dataset.append(row, ignore_index=True)\n",
    "\n",
    "                    # add Station ID\n",
    "                    df_dataset.iloc[line][\"StationID\"] = Stations[\"StationID\"][St]\n",
    "                    # add Lat and Long\n",
    "                    df_dataset.iloc[line][\"Lat\"] = Stations.loc[St, \"Lat\"]\n",
    "                    df_dataset.iloc[line][\"Long\"] = Stations.loc[St, \"Lon\"]\n",
    "                    # add date of measurement\n",
    "                    df_dataset.iloc[line][\"Date\"] = dat\n",
    "                    # add station elevation\n",
    "                    df_dataset.iloc[line][\"Elev\"] = elevations.loc[St, \"Elev\"]\n",
    "                    # add snow bulk denisty\n",
    "                    # df_dataset.iloc[line]['Den'] = CHSS['Den(kg/m3)'].iloc[St, dat]\n",
    "                    # add SWE\n",
    "                    df_dataset.iloc[line][\"SWE\"] = SWE.loc[St, dat]\n",
    "                    # add snow depth\n",
    "                    df_dataset.iloc[line][\"SD\"] = SD.loc[St, dat]\n",
    "                    # days without snow since 1st of august\n",
    "                    df_dataset.iloc[line][\n",
    "                        \"days without snow\"\n",
    "                    ] = nb_days_without_snow_vect.loc[dat][0]\n",
    "                    # print(nb_days_without_snow_vect)\n",
    "                    # number of frost-defrost events since 1st of September\n",
    "                    df_dataset.iloc[line][\"number frost-defrost\"] = frost_defrost_vect.loc[\n",
    "                        dat, 0\n",
    "                    ]\n",
    "                    # calculate the accumulated temperature from 1st of September till record\n",
    "                    df_dataset.iloc[line][\"accum pos degrees\"] = pos_degree_vect.loc[dat][0]\n",
    "                    # calculate the average age of the snow cover\n",
    "                    # Try/except to deal with division by zero that can occur\n",
    "                    try:\n",
    "                        (\n",
    "                            df_dataset.iloc[line][\"average age SC\"],\n",
    "                            total_precip_mod_solid,\n",
    "                            cumul,\n",
    "                            nb_days,\n",
    "                        ) = _input_calc.age_snow_cover(dat, tmid, total_precip_solid)\n",
    "                    except:\n",
    "                        continue\n",
    "                    # add number of days since 1st of September\n",
    "                    df_dataset.iloc[line][\"DOY\"] = nb_days\n",
    "                    # estimate the number of layers in the snow cover\n",
    "                    df_dataset.iloc[line][\"number layer\"] = num_layer_vec.loc[dat][0]\n",
    "                    # calculate accumlated solid precipitation from 1st September till record\n",
    "                    df_dataset.iloc[line][\"accum solid precip\"] = cumul\n",
    "                    # calculate accumlated solid precipitation in the last 10 days before the record\n",
    "                    df_dataset.iloc[line][\"accum solid precip in last 10 days\"] = np.sum(\n",
    "                        total_precip_mod_solid[-10:]\n",
    "                    )\n",
    "                    # calculate accumlated total precipitation in the last 10 days before the record\n",
    "                    dat_Ndays = pd.to_datetime(dat) - timedelta(days=10)\n",
    "                    dat_Ndays = (\n",
    "                        str(dat_Ndays.year)\n",
    "                        + \"-\"\n",
    "                        + \"{:02d}\".format(dat_Ndays.month)\n",
    "                        + \"-\"\n",
    "                        + \"{:02d}\".format(dat_Ndays.day)\n",
    "                    )\n",
    "                    df_dataset.iloc[line][\"total precip last 10 days\"] = np.sum(\n",
    "                        total_precip.loc[St, dat:dat_Ndays:-1]\n",
    "                    )\n",
    "                    # calculate average temperature in the last 6 days before the record\n",
    "                    dat_Ndays = pd.to_datetime(dat) - timedelta(days=6)\n",
    "                    dat_Ndays = (\n",
    "                        str(dat_Ndays.year)\n",
    "                        + \"-\"\n",
    "                        + \"{:02d}\".format(dat_Ndays.month)\n",
    "                        + \"-\"\n",
    "                        + \"{:02d}\".format(dat_Ndays.day)\n",
    "                    )\n",
    "\n",
    "                    df_dataset.iloc[line][\"average temp last 6 days\"] = np.mean(\n",
    "                        tmid.loc[dat:dat_Ndays:-1]\n",
    "                    )\n",
    "\n",
    "                    # add snowclass\n",
    "                    df_dataset.iloc[line][\"snowclass\"] = Stations.loc[St, \"Class\"]\n",
    "                    # increase line count\n",
    "                    line += 1\n",
    "\n",
    "\n",
    "    df_dataset.to_csv(\"USA_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow survey data and meteorological data loaded\n",
      "2022-11-11 02:33:31.779514\n",
      "200 left till loop over stations done\n",
      "Data points created:  238\n",
      "2022-11-11 02:34:05.560793\n",
      "175 left till loop over stations done\n",
      "Data points created:  675\n",
      "2022-11-11 02:34:37.935239\n",
      "150 left till loop over stations done\n",
      "Data points created:  1004\n",
      "2022-11-11 02:35:09.979812\n",
      "125 left till loop over stations done\n",
      "Data points created:  1290\n",
      "2022-11-11 02:35:41.977475\n",
      "100 left till loop over stations done\n",
      "Data points created:  1563\n",
      "2022-11-11 02:36:13.831900\n",
      "75 left till loop over stations done\n",
      "Data points created:  1819\n",
      "2022-11-11 02:36:46.013712\n",
      "50 left till loop over stations done\n",
      "Data points created:  2084\n",
      "2022-11-11 02:37:19.351826\n",
      "25 left till loop over stations done\n",
      "Data points created:  2477\n",
      "2022-11-11 02:37:51.528149\n",
      "0 left till loop over stations done\n",
      "Data points created:  2764\n"
     ]
    }
   ],
   "source": [
    "Stations = pd.read_csv(\"USA_Stations.csv\")\n",
    "SWE = pd.read_csv(\"USA_SWE.csv\",index_col=False)*10\n",
    "SD = pd.read_csv(\"USA_SD.csv\",index_col=False)\n",
    "total_precip = pd.read_csv(\"USA_data_tp.csv\", index_col=False)*1000\n",
    "tmin = pd.read_csv(\"USA_data_tmin.csv\", index_col=0)\n",
    "tmax = pd.read_csv(\"USA_data_tmax.csv\", index_col=0)\n",
    "savefile = \"Dataset.csv\"\n",
    "\n",
    "elevations = pd.read_csv(\"USA_Elevations.csv\")\n",
    "\n",
    "\n",
    "feature_calculations(SWE, SD, total_precip, tmin, tmax, Stations, savefile, elevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that will be used are XGBoost and Random forrest regressor, as well as two linear regressiond developed by Sturm et al. and Jonas et al. \n",
    "\n",
    "Sturm et al.:\n",
    "https://journals.ametsoc.org/view/journals/hydr/11/6/2010jhm1202_1.xml\n",
    "\n",
    "Jonas et al.:\n",
    "(https://jannefiluren.github.io/docs/Jonas2009%20-%20Estimating%20the%20snow%20water%20equivalent%20from%20snow%20depth%20measurements%20in%20the%20Swiss%20Alps.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step before running algorithm, designating 30% of the stations to be used for validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2764, 18)\n",
      "103 30\n",
      "(777, 18) (1987, 18) 2764\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"USA_Dataset.csv\", index_col=False).dropna()\n",
    "print (df.shape)\n",
    "unique = list(df[\"StationID\"].unique())\n",
    "\n",
    "val_unique = random.sample(unique, int(len(unique) * 0.30))\n",
    "val_df = pd.DataFrame()\n",
    "print(len(unique), len(val_unique))\n",
    "i = 1\n",
    "for x in val_unique:\n",
    "    adf = df.loc[(df.StationID == x)]\n",
    "    df = df.loc[(df.StationID != x)]\n",
    "\n",
    "    val_df = val_df.append(adf)\n",
    "    i = i + 1\n",
    "\n",
    "print(val_df.shape, df.shape, val_df.shape[0] + df.shape[0])\n",
    "\n",
    "val_df.to_csv(\"test.csv\", index=False)\n",
    "df.to_csv(\"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series(\n",
    "            [\n",
    "                X[c].value_counts().index[0]\n",
    "                if X[c].dtype == np.dtype(\"O\")\n",
    "                else X[c].median()\n",
    "                for c in X\n",
    "            ],\n",
    "            index=X.columns,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def xg(params, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Takes a list of model parameters, training X values, and training y values as input and returnes a\n",
    "    trained XGBoost model.\n",
    "    \"\"\"\n",
    "    if len(params) != 4 or params == None:\n",
    "        params = [9, 1000, 0.035, True]\n",
    "        print(\"Using default xg parameters\")\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth=params[0],\n",
    "        n_estimators=params[1],\n",
    "        learning_rate=params[2],\n",
    "        tree_method=\"gpu_hist\",\n",
    "        single_precision_histogram=params[3],\n",
    "    ).fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def rf(params, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Takes a list of model parameters, training X values, and training y values as input and returnes a\n",
    "    trained random forest model.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(params) != 7 or params == None:\n",
    "        params = [60, None, 12, 3, 1, True, True]\n",
    "        print(\"Using default rf parameters\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params[0],\n",
    "        max_features=params[1],\n",
    "        max_depth=params[2],\n",
    "        min_samples_split=params[3],\n",
    "        min_samples_leaf=params[4],\n",
    "        bootstrap=params[5],\n",
    "        oob_score=params[6],\n",
    "    ).fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_model(method, df, feature_columns, target, test=[], params=None):\n",
    "    \"\"\"\n",
    "    Function takes m*n pandas dataframe (df) and method (with optional parameters) as inputs and gives\n",
    "    predictions as output.\n",
    "    feature columns (lst) = list with length m with names of columns containing predictors\n",
    "    target (str) = name of target variable\n",
    "\n",
    "    Returns 1xn pandas dataframe of predicted values\n",
    "    \"\"\"\n",
    "    big_X = df[feature_columns]\n",
    "    # big_X_imputed = DataFrameImputer().fit_transform(big_X)\n",
    "    big_Y = df[target]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    big_X_imputed = big_X[feature_columns].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "    if isinstance(test, pd.DataFrame) != True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            big_X_imputed, big_Y, test_size=0.33, random_state=0\n",
    "        )\n",
    "    else:\n",
    "        X_train = big_X_imputed\n",
    "        X_test = test[feature_columns].apply(LabelEncoder().fit_transform)\n",
    "        y_train = big_Y\n",
    "        y_test = test[\"SWE\"]\n",
    "\n",
    "    # ___________________________\n",
    "    start = time.time()\n",
    "    if method == \"xg\":\n",
    "        model = xg(params, X_train, y_train)\n",
    "\n",
    "    elif method == \"rf\":\n",
    "        model = rf(params, X_train, y_train)\n",
    "\n",
    "    time2compute = time.time() - start\n",
    "    # ______________________________\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    \"\"\"print(\n",
    "        \"Calculation finished in \"\n",
    "        + str(time2compute)\n",
    "        + \" using \"\n",
    "        + method\n",
    "        + \" with an R2 score of \"\n",
    "        + str(r2_score(predictions, y_test))\n",
    "    )\"\"\"\n",
    "\n",
    "\n",
    "    return predictions, r2_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsCUlEQVR4nO3deZxkZX3v8c+vtq7qfV+mu2cfYUYzzmhnEJRF3NAbRSBRiBo1uZJ7lVyXcPMCoyZgfGEiiUtCjARRySUSY2Je3MTr8gIUEFF6mIVlYDagl5nprt6mt6qu7Xf/OKeqq7p7eqpneqieM7831Ouc85znnHq6p/p7nnrOqVOiqhhjjPEuX6kbYIwx5syyoDfGGI+zoDfGGI+zoDfGGI+zoDfGGI8LlLoBczU2NuratWtL3QxjjDmr7Ny5c0hVmxZat+KCfu3atXR3d5e6GcYYc1YRkZdOtM6GbowxxuMs6I0xxuOKCnoRuUJEnheRgyJy0wLr14jIAyKyV0R+JiIdeevSIrLbfdy/nI03xhhzcicdoxcRP3AH8BagD3hCRO5X1Wfzqt0O3KOq3xGRy4HbgA+462Kqum15m22MMaZYxfTodwAHVfWwqiaA+4Ar59TZAjzozj+0wHpjjDElUkzQtwO9ect9blm+PcDV7vxVQJWINLjLYRHpFpHHReTdp9NYY4wxS7dcJ2NvBC4VkV3ApUA/kHbXrVHVLuB3ga+IyIa5G4vI9e7BoDsajS5Tk4wxxkBx19H3A515yx1uWY6qHsHt0YtIJXCNqo656/rd6WER+RmwHTg0Z/s7gTsBurq67L7JxphzQmZqikRfP8m+XhK9vfgi5dS99z3L/jzFBP0TwCYRWYcT8Nfi9M5zRKQRGFHVDHAzcLdbXgdMq+qMW+f1wF8tY/uNOSM0lSI1PExqYIDkwACpgUFSg4MEGhuJbN9G+PzzkWCw1M00K5xmMqSiUZI9PSR6+9xA7yPZ6wR7eni4oH5k27bSBL2qpkTkBuDHgB+4W1WfEZFbgW5VvR+4DLhNRBR4GPiYu/lm4BsiksEZJvrinKt1jHnZpScnSQ0MuCHuBHhqYIDkoBvoAwOkhoYgkync0OfLlUk4TORVryKyfbv72Eagrq4EP83Ko6rEUjFG4iNUhaqoKaspdZPOqMz0NIm+PpJ9boD39JLo6yXZ65RpIjFb2ecj2NZGsLOTqssvJ9jZSaizg2CHM/XX1p6RNspK+4aprq4utVsgmFOxYC98YIDUoBvobrhnpqfnbeurribY0kyguYVASwuBlmaCLS3usjPvr68nNTBAbPdupnftIrZrN/F9+yCVAiC0dm0u9CPbtlG2cSPiO/s/k6iqTCWnGI2PMjIz4kzjI4zEnfm55aPxUWbSM7nt2yvb2Vy/mfPrz2dzw2a2NGyhMdJYwp9oaZxe+RDJ3p7Z3rgb5IneXtJDQwX1fZWVBFd3EuroJNjZQaiz0w30ToJtbWfsnaCI7HTPh85fZ0FvVopkJkk8FSeeipPWNIIgIgiCTk2jg0NkokPOdDBKOupO3UdmeGR+LzwQINDURKClmUBzM4GWFoLNLQRbnRAPtjhlvkjklNqcicWIP/NMLvhju3aRHh0FwFdVReTVryayfRvl27cT3roVf2Xl6f6aTpuqMpGcyIX0cHx4NrDjI4zOjM5bl8wkF9xX2B+mPlxPXbiOunCdM182Oz8SH+G5kefYN7KPl8Znb8XSGGlkc/1mJ/jrt3B+w/msqliFiLxcv4YCmViMZF/f7PBKT68b6G6vfGb2wIXPR7C1lWBnNshXO73yzk6CHU6vvBQ/hwW9OW3pTJp4Ok4sFSOWihFPxXPTeDrOdGo6F9K59ekYsWSMeDpesC6WnrN9MkZwPEbTSIrWEaV1VGkch/oJqJ9Q6ichkpjfpskwjFTCSJUwWjU7P1I1Ox0vB13kj84nPueAguD8L7Nl7nbZeR8+/D4/IV+IoD9I0Bck4AsQ9AUJ+oNOuQRoHEnR/uIkq14Yp+XQGLVHxhF12jHRWcfxV7QxdV4H05s7ybQ1EfSHCPqChNxpdn/ZfReU5z+XOx/0BXNDJfN62zOjBeXZslQmteDvozxQPhvY4Trqypz5eWHurisPls/bh6bTpKJRUseOkZ6cAlVAiSWm6Zvso/d4D30TvfSO9zA4NUBGMwhQ7i+no6qDzop22ivb6ahspzHciA/QjOb2gzoPVQUlt5xdp9nl7Lq55RklPTaWGy9P9PaQjs7plVdUzPbC84dXVru98lCoiL+al5cFfYmoKuM//CHRL38FMhmCHR3uo51Qdr69g0BT4xl9i59IJxiODTMSH2E4PsxwbJjhuLM8lZxaMLjnBvqJenSLKfOXEQ6ECfvDRAIR6meCtI1Cy0iGxqEk9UMz1A5OUzU4STA2u3/1+UjWV5FqqCJZX02yoYpkfRWJhioS9ZUk6qtI1leSLguS0QyK8xpWVbL/ZTRTWKZKhozzN89s2YLT7PzcZZRUJkUqkyKZSZJMJ0lkErn5ZMZ5JNJOWbaef2qGjp5p1vbMsKE3yYb+DOXugWusHPZ3CM+3C/s7hMOtkAwsX2+wMlg5G9BlhWFdEN7uunAgvOj+NJMhPTJC8ugxkseOkjp6jOSxY6SOHXXLjpEaHIR0etH9lJwIgbbWhYdXOjtL1is/HYsF/Yq7TbFXJPr6OHbLrUw98gjhLVsIbdhAsq+PqUceITXnswJSVkawvX32ANBeeEDw1xSezFJVplPTs4Edmx/guWCPDTORnFiwjZFAhKpgFZFghLA/TDjgBHJtuJaIPzKvPBJwlk9UHg6EKYulCPRHofcoqZ5eEi+9lHtkjh+ffXL3pFRozWZCr19DaM0agmucaaijw9NXtGRSKWIH9zO5cyeR3btp2L2XHQ/1OSsDAThvPZlXvYLklvUktqxjpq4idxCZe1DJHmwigciCYR7yF9/zVHV7useOkTx6lNSxYwsE+jE0WXjQl1CIQFsrwdY2KnbsyM0H21rxVVVB9t2SCOQ/yM4Xrktpmr7JPg4ff4HDx1/g0PhhDo8fJuaO+wf8QdbUrGV97QY21m1kY+0m1tauIxwoK9x/7jnmP7evshLfCuyVnynWo19mmkox8p17iP7t34LPR/MnPk7d+96H+P25Opl4nGR/vzMm2NdHsq+fRF8v8d4eUv39MDFVsM9EJMjxhjKG6/wcrc7QWzlDf3WKwVohWgOJ4GzPo6ashoZwAw2RBurD9bn5hrC7HJldFwmc2rh0enKKxEsvksyG+IuzYZ4dn84KtLU54Z19rHVDvbPznPpDO5nU8DCxPXuI7drF9K5dxJ96OjcuHFy1quDqnvB55yGBpffR0pNTeT3vhXvjGosVbhQIEGxuJtDW5oxLt7UScEM80NpKsK0Nf13dGe/9pjNpeiZ62De8j+dGnuPZkWfZN7yP8cQ4AH7xs65mHVsatuRO/J5ffz6VodKfE1lMOpNmJj1DPB1nJjWDiNBa0XpK+7Khm5dJ7KmnOPq5P2Nm3z4q3/hGWj/7GYKrVtE73kvPRE+u153taef3vkfjo6TUGTctjyvNY9B8XGk9LnRMlNE27qNxLEPNcJxAcs4Jx4ZaAu3thDvXUNbZOTss1NFBsLX1lEIhMz1NoqenIMRzYT7nKoNAc3NhiGdDffVqfOHFhwLMwjSRIP7cc07w795N7MldpAYGAJBIhMjWrUS2bXNO9G7bhoTDTg/c7YUvFOiZiTnv7EScE9XZHnhra0FvPNDaRqCxoaCTspKoKkenjrJveF8u+J8beY5obPYd85rqNc7VPu6J3831m6kLn/gy2IxmiKfiTvi6w5jZ+ROVzaRniKVizKRmcuuy87nt3CDP3zaejs87V7K1aSv3vuPeU/p9WNCfYenJKaJf/Sqj995LoKGBls98hqq3voW9Q3v5x73/yM/7fl5QP+QL5XrZuZ733GV3vqasBp/Mjt+rqvMBjL5+kv19Be8Kkn19JI8dKxwf9fudnljBuYFOgh3tBFe1kz4+RuKll+b1zlODgwVt9jc2FvbMs8G+ejW+8vkn5MzySx496vb4nat74vv2LToW7q+vd8N7fm882NpKoLnZk0Nk0eko+0ac0N83vI99I/von5z9MH9rRSsN4YbC4HZD+FTORYFzwj4cCBeclyrzl1EWKMvN58oXKWsqb+IN7W84tTZY0J85Ew8+yLFbP09qYIC6666l8ROfYNf089y5904eP/o4NWU1vH/z+9nRuiMX5hXBijP2VldTKadX1zf/IJDo75t3dUE+f319XpCvLuidr4TLAk2hTCxG/Omnmd69G9JpZyglb1jFV1ZW6iauGMdnjueC/9mRZ5lITBAJRJwwdgN3bkgvWpY9J+UGdNAXLPnJWwv6MyA5MMDAX3yBiZ/+lLJNm2i95RaebJnizr13smtwFw3hBj70yg/xnvPes+AlaKWSicVIHjniBn8//qrq3JCLv7q61M0zxpwiu+pmGWk6zeh99xH9my+jqRSNn/wEe9+yjpuf/SLPPvUsrRWtfPqCT3PVxqtOeqlaKfgiEco2bKBsw7ybiBpjPMqCfgniz+/n2Oc+R2zPHsovfB3P/8EbuXn4Bxx85CCdVZ3cctEtvHP9Own6vTfuaYw5e1nQFyETjzP0919n+O678VVVceRTv8OXG3fy0qEvsb5mPbddfBtXrL2CgM9+ncaYlceS6SSmHnuMo39+C8meHkYu38ZfXxDlAD9gc3AzX77sy1y++vKCq2KMMWalsaA/gdTICANf/CLj9/9f4m11fOND9fyi7Wm2Nm3ljq2f4+L2i0t+lt0YY4phQT+HqnL833/AwF/9JanJSf7fJeX88wXjbO+4gLu2Xs+O1h0W8MaYs4oFfZ6ZF16g73OfJfHETg50+vn67whrt+3gm1uvZ3vz9lI3zxhjTokFPc7HzXu+/jUm7vo2cV+ae6/wwTvfzO3bPsIrG155+k8wdAAyaahshkide7MlY4x5eZzzQd/36E/p/9xnqT5ynF9v9tHz4bfysYv/iI11G09vx6pw4Kfwi6/CS4/OlvuCTuBXNEFlC1S604pmp7yy2V1ugnCNHRSMMaftnA36nr5n2fv5/82Gnx9mphp+/vE38I73fZbV1atPb8epBDz9b/DY12DwWahuh7f+BVS1weQgTA0608lBmDgKR/fAVBR0gXuW+MvmHAyyBwf3gJB/cAhV2kHhRDJpSCcgnYRMyp0mF1hOOfVOtC7jlmXSzr9XJgOacefTedPM/OVc/bl1l7qfuWXutKxqtpOQm7bMvk4qmsB/zv65z1KFxJTz7xiMgD90TvzdnHP/8gdHD/LAt25h673drJuGA2/bzGv/9Etc0nyanxSdmYCd34HH/x7G+6F5C1z1DXjVNXCyD1BlMhAbcQ8AA07wTw7MHhCmBmGsB/q6nXUscNuKYPniB4L8P/rQKd6SIZNxA9MNzWLns8G55G3nhOyiwZxfZ87yQr+vl4WAzw/iB/HNzvt87tSfN/XNqbNIXZ8fJFi478QkHHsKJh+AmfGF21LeMOdA0Fz4eskeHM6W4cV0CuJjMD3i/P2ccDpauJzO/7oygUAYAmVO8AfK3GX3EQwXLhfUi5zCdnllL+P3CZ8zQb9veB/f/dnXOO/un3PJIWVsXSP1f3Ebr3ztqd0pLmdiAH71D/DEN2HmOKy9GN75Vdj45uL/WHw+qGh0Hi1bFq+bTsH0sPvOYAAmo/MPDsOHoOeXTr2FhKqcdwkVzU5YLBi0qfllC73rWA6+gNOz8gedqS/ozgfd+cBsmS/oHKjm1Qm6+1lk+UT7y18u2H5u3YD7KCag/aULy2RstpMwOZDXacibjhxyXrt5X+Kdkx1ezB9GzH+HkD8tW4ab3alCcvoEQT164gCPHz/xPn0BiNRDeb0zrV8P7a+dXfYHIRWHZNyZZh+55RlIxZzf5fTI7HJqZrbOQr+7pfCHZg8C2YPGqm1wzV2nt98FeD7odw/u5q7d36DqPx7mvY9k8PsCVP3xRzn/wx85pfu05wwdgMf+FvZ81wnDLe+Ciz4OHa9dvsYvxB+AqhbnwW8sXjedhKmh2T/whQ4Oqs6wT37QLjgfLKLOKcz7gi9rz+acEIxA3RrnsRhVp/d/wgPCIIwfgSO7ndeOZubvI1iRd1Bonn9AgOICfLHQDFVBed1scNetmw3s3LSucLms6swfaDMZp90LHTBSM85BouAAkV1e6MDiPqrbz0hTPRn0qkr3QDff2PsNBp98nI/9CFYfyxC+5A10/PktBFetOvWd9z4Bv/gKPPdfzhF4+/vhwhugYQXeJMwfhOo252HMXCLOCf9wDTRuWrxuJu0E8kIHhGwHYugAvPioE+ILPp+/MKDr1kL79jmBPWcaqYPACv0mMp8PfBHnwHpqX9b2svFU0Ksqj/Y/yp1772Rf/y4+9FgZl/8qQ6ChgdavfIaqt7311D7slMnAgZ84V9D0PAbhWrjkRtjxh84QiDFe5/O7FwU0Aa9avG5qZnYoEWaDu6z67Bj79yDPBH3/ZD+ffOiT7BvZx5t7a/jmjyoIDY1Te921NH/qU/irqpa+01QCnvpX5wqa6HNQ0wlXfBG2f2B5xiaN8aJAGdR0OA+zIngm6JsjzbTGyvjjh8+n+hdPU7ZpI61f+wfKX3MKn2iNj8POb8PjX4eJI9DyG3D1XfDKd5/8ChpjjFlhigp6EbkC+CrgB+5S1S/OWb8GuBtoAkaA96tqX976auBZ4D9U9YZlanuBTE8fH/ur59BEgsZPfJyG3/99JLTEsb2JY064d9/tnKRadylc+Xew4XJ7y2mMOWudNOhFxA/cAbwF6AOeEJH7VfXZvGq3A/eo6ndE5HLgNuADees/Dzy8fM2eL7R2LXXXvpfa3/5tQmvXLm3j6H5neGbvvzjXaW95N7z+f8Equ7+NMebsV0yPfgdwUFUPA4jIfcCVOD30rC3Ap9z5h4D/yK4QkdcCLcCPgAW/z3A5iAjNN964tI16fuWcYH3+v5zrWF/zQbjwY1C/7sw00hhjSqCYoG8HevOW+4AL5tTZA1yNM7xzFVAlIg3AKPDXwPuBN5/oCUTkeuB6gNWrT/MWBCeTycD+HzkB3/u4c/nWpTfBjo84H1gyxhiPWa6TsTcCfyciH8IZoukH0sBHgR+qat9ilzWq6p3AnQBdXV1n5vPqqRnY+z1niGZoP9Suhrd/Cba/D0IVZ+QpjTFmJSgm6PuBzrzlDrcsR1WP4PToEZFK4BpVHRORC4GLReSjQCUQEpFJVb1pWVpfjPhx6P6Wc5J18hi0boVrvumMw9tNnowx54Biku4JYJOIrMMJ+GuB382vICKNwIiqZoCbca7AQVXfl1fnQ0DXyxby40fcK2i+BYkJWP9GuOofYP1ldgWNMeacctKgV9WUiNwA/Bjn8sq7VfUZEbkV6FbV+4HLgNtERHGGbj52Btu8uMHnnHvQ7P0X5yZcr7zauYKm7dUla5IxxpSSqJbqFq4L6+rq0u7u7qVvOH4E/vOTzonWQARe83tw4Ued+2kYY4zHichOVV3wykbvDFKHa2HkBbjs0/Cb/x0qGkrdImOMWRG8E/ShcvjYr2z83Rhj5vDWjcAt5I0xZh5vBb0xxph5LOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjLOiNMcbjigp6EblCRJ4XkYMictMC69eIyAMisldEfiYiHXnlT4rIbhF5RkT+x3L/AMYYYxZ30qAXET9wB/B2YAtwnYhsmVPtduAeVd0K3Arc5pYfBS5U1W3ABcBNIrJqmdpujDGmCMX06HcAB1X1sKomgPuAK+fU2QI86M4/lF2vqglVnXHLy4p8PmOMMcuomOBtB3rzlvvcsnx7gKvd+auAKhFpABCRThHZ6+7jL1X1yNwnEJHrRaRbRLqj0ehSfwZjjDGLWK4e9o3ApSKyC7gU6AfSAKra6w7pbAQ+KCItczdW1TtVtUtVu5qampapScYYY6C4oO8HOvOWO9yyHFU9oqpXq+p24E/dsrG5dYCngYtPp8HGGGOWppigfwLYJCLrRCQEXAvcn19BRBpFJLuvm4G73fIOEYm483XAG4Dnl6vxxhhjTu6kQa+qKeAG4MfAPuB7qvqMiNwqIu9yq10GPC8i+4EW4Atu+WbgVyKyB/g5cLuqPrXMP4MxxphFiKqWug0Furq6tLu7u9TNMMaYs4qI7FTVroXW2eWOxhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcUUFvYhcISLPi8hBEblpgfVrROQBEdkrIj8TkQ63fJuI/FJEnnHXvXe5fwBjjDGLO2nQi4gfuAN4O7AFuE5Etsypdjtwj6puBW4FbnPLp4HfU9VXAlcAXxGR2mVquzHGmCIU06PfARxU1cOqmgDuA66cU2cL8KA7/1B2varuV9UD7vwRYBBoWo6GG2OMKU4xQd8O9OYt97ll+fYAV7vzVwFVItKQX0FEdgAh4NDcJxCR60WkW0S6o9FosW03xhhThOU6GXsjcKmI7AIuBfqBdHaliLQB/wR8WFUzczdW1TtVtUtVu5qarMNvjDHLKVBEnX6gM2+5wy3LcYdlrgYQkUrgGlUdc5ergf8C/lRVH1+GNhtjjFmCYnr0TwCbRGSdiISAa4H78yuISKOIZPd1M3C3Wx4CfoBzovb7y9dsY4wxxTpp0KtqCrgB+DGwD/ieqj4jIreKyLvcapcBz4vIfqAF+IJb/h7gEuBDIrLbfWxb5p/BGGPMIkRVS92GAl1dXdrd3V3qZhhjzFlFRHaqatdC6+yTscYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43EW9MYY43FFBb2IXCEiz4vIQRG5aYH1a0TkARHZKyI/E5GOvHU/EpExEfnP5Wy4McaY4pw06EXED9wBvB3YAlwnIlvmVLsduEdVtwK3ArflrfsS8IHlaa4xxpilKqZHvwM4qKqHVTUB3AdcOafOFuBBd/6h/PWq+gAwsQxtNcYYcwqKCfp2oDdvuc8ty7cHuNqdvwqoEpGGYhshIteLSLeIdEej0WI3M8YYU4TlOhl7I3CpiOwCLgX6gXSxG6vqnarapapdTU1Ny9QkY4wxAIEi6vQDnXnLHW5Zjqoewe3Ri0glcI2qji1TG40xxpyGYnr0TwCbRGSdiISAa4H78yuISKOIZPd1M3D38jbTGGPMqTpp0KtqCrgB+DGwD/ieqj4jIreKyLvcapcBz4vIfqAF+EJ2exF5BPhX4E0i0icib1vmn8EYY8wiRFVL3YYCXV1d2t3dXepmGGPMWUVEdqpq10Lr7JOxxhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcUUFvYhcISLPi8hBEblpgfVrROQBEdkrIj8TkY68dR8UkQPu44PL2XhjjDEnd9KgFxE/cAfwdmALcJ2IbJlT7XbgHlXdCtwK3OZuWw/8GXABsAP4MxGpW77mG2OMOZlievQ7gIOqelhVE8B9wJVz6mwBHnTnH8pb/zbgp6o6oqqjwE+BK06/2cYYY4pVTNC3A715y31uWb49wNXu/FVAlYg0FLktInK9iHSLSHc0Gi227cYYY4qwXCdjbwQuFZFdwKVAP5AudmNVvVNVu1S1q6mpaZmaZIwxBiBQRJ1+oDNvucMty1HVI7g9ehGpBK5R1TER6Qcum7Ptz06jvcYYY5aomB79E8AmEVknIiHgWuD+/Aoi0igi2X3dDNztzv8YeKuI1LknYd/qlhljjHmZnDToVTUF3IAT0PuA76nqMyJyq4i8y612GfC8iOwHWoAvuNuOAJ/HOVg8AdzqlhljjHmZiKqWug0Furq6tLu7u9TNMMaYs4qI7FTVroXW2SdjjTHG4yzojTHG4yzojTHG4yzojTHG4yzojTHG4yzojTHG4yzojTHG4yzojTHG44q5140xZ5SqMh5L0T8W48hYjKPjcTpqI1ywvp7ykL1EjTld9ldkzrhEKsOx4/FckB8Zi3HkeHx2fizGVGL+zU5Dfh9da+u45BVNXLypkc2t1fh8UoKfwJizm90CwZwWVWV0OsmRsVhhkI/NBnt0coa5L7PGyjJW1YZZVRNhVW2EVbVh2mud+ZbqMAcGJ3jkwBAP74/y3LGJ3DYXb2rk4k2NvGFTI81V4RL8xMasTIvdAsGC3iwqnkxz1O19zw1yp2ceI57MFGwTDvpYVRtxgnuBIG+tCRMO+otuw8B4nEcPDPHwgSiPHhhieCoBwOa2ai7Z1Mglr2jitWvqlrRPc+6Zmknx6xdHePzQMDOpDJtaKtnUXMWm5krqKkKlbt5ps6A3C8pklOGpRC68++cE+JGxGEOTiXnbNVeVzQZ5bdgN8kguyOvKg4icmSGWTEZ59ug4Dx+I8sj+IbpfGiGZVsJBH69b38DFm5q4ZFMjG5srz1gbzNkhnkzzZM8ovzw0zGOHhtnTO0Yqo4T8PgJ+YTpvuLCxMsSGpsqC8N/YUklTZdlZ8zqyoD+HjceT9I5Mu48YPSPT9I5O0zMyTd9ojESqsDdeHvLnAtsJbyfI22qcIG+pKaMssHJ6zlMzKX71wjAP73d6/IejUwC01YS52O3tv35Doyd6bGZxyXSGvX3H+eWhIR47NEz3S6MkUhl8Als7arloQwMXbWh03/35OHI8zoGBCQ4OTnJwcJIDg5PsH5hgIp7K7bMmEmRTs3MA2NhcxcbmSjY1V9JWE15xBwALeg9LpDL0j8XoHZnOhXhfXqCPTScL6leHA6xuKKezrpzO+nI66maHV9prI1RHAivuBbwUfaPTPHJgiEfcYZ7xeAoR2Npe457UbWL76lqCfruy+GyXfXfn9NiH+PULI7mT+pvbqt1gb+A319VTHQ4WtU9VJToxw4HBSQ4MTDhT90AwMjX77rayLMAGN/SzB4JNzVW010ZKdsGABf1ZLPvCy/bCc71y93FsPE4m758w5PfRUReho76c1fUROuvKWV3vhHpnXTk15cW94L0gnVH29I3xiNvb3907RjqjVJYFuHBDQ258f01DRambuiTxZJroxAxDkzMMTSYYmpyhoizA+sYK1jZWUFnmzYvpVJVD0UkeOzTMYweHefyF4VxHZn1TRa7H/rr1DdSfgXdww5MzuZ6/M53gwMAkgxMzuTrhoI+NzZVsbKpkU8vsO4DV9eUEznDnwoJ+hZuIJ+kdidE7OhvgvaMxd3hlet7JzpbqMie868rdQC+nsy7C6oZyWqrCdgniCRyPJfnloWEePhDl4f1R+kZjAKyuL88N81y4oaHo3t9ymk6kGJpIEJ3MBvgMQxOJ2fnJGTfcE0zOpBbdV1NVGesaKljXWMG6pgrWNlSwvqmC1fXlZ90J696RaR5zh2IeOzRM1A3V9tqIE+wbG7hwfSOtNaW7Auv4dJKDUSf0c+8ABiY4cjyeqxPy+1jfVOEGf5X7DqCSNQ0VhALLcwCwoC+xZDrDkbHY/DFyd7hldM7wSlVZwOmB10dme+PZYK+LnHV/rCuRqvLi8DSPHIjy8P4hfnloiKlEGr9PeM3qWi7e5Fy7v7WjFv8pHjinZlJ5IZ3IC+vC3vjQxMyCnyMAZ4y4sTJEY2UZTVVleVOnrLGyjMaqMibjKV4YmuSFoWl3OsULQ9MMTc72NkVgVU2E9W74r2ucfXTURc54j7MYA+Px3FDMY4eGcwfjxsqy3FDMRRsa6ayPrPghxsmZFIcGs+E/wUH3QNA7Op273DjgE9Y0lOfC/1XtNbztla2n9HwW9MsonkwzNp1kZCrB2HSC0ekkI9MJxqac+dHphPtIMjrlzOef3AEI+oX22khBgK/OC/aayJm7asUsLJHKsKtn1Lma58AQT/UfR9UJ2jdsbOSSVzRy8aYmqiPB2bB2p9G8wI7m9cZjyYXDu648WBDS+UHelCsP0VBRdtq9vfF4kheHptzgn8rNHx6aKnhdBnzC6vryXPCvbazIDQW1Vp+5d4mjUwkePzzs9tiHOOSeTK+JBHnd+nou2tDIRRsaPHUVVSyR5lC0cPjnYHSSl4an2d5Zy/f/50WntF8L+gWoKlOJdC6MR6eTjE0nGJkqnB9zwzsb7if64wXnBE1teZC68hB1FSHq3Pna8iCramd7563V4VPuJZqXx8hUgkcPDvHI/igPH4gyMD5zwroiUFceckK6Kq+nXemGuBvgTVVl1FeEVsSJYFVlZCqRC/2Cg8HwVMFwYTjoK3gHkD0IrGusoL4itKQAnogneeLFER476IT7s0fHAedqrx3r6nM99s1t1efc38hMKs3x6STN1ac2DHVOBH0qnaF3NOaE9tTJg3t0OkEyvfDPLgLV4SD1FaHZ4C53g7tidr62PES9G+g15cEVddmhWT6qyoHBSR49MEQynSnoiTdVOuG9EoY9lksmoxwbj/OiexDIfyfQMzJNKu/sf1V49iRw/lDQ2sYKqsNB4sk0O18azQ3F7O07TjqjhAI+Xru6LjfOvrXDroQ6XedE0A9OxNnxhQfmlft9kutZZ3vXToDP9rizve9scNdEgudcb8KYYqTSGfpGY7nef/YdwOHoFEeOxwpuddFYGWI8liKRzuD3Ca/uqMkNxbzGPsm87BYLes9ch1VXHuJv3vPqgh53XUWIqrKz+7pwY1aSgN/HWrfH/sY56+LJND0j0xyOzr4LqI4EuGhDI7+5rt6zl32eDTzzmw/6fVz9mo5SN8OYc1Y46OcVLVW8oqWq1E0xc9igmDHGeFxRQS8iV4jI8yJyUERuWmD9ahF5SER2icheEXmHWx4SkW+JyFMiskdELlve5htjjDmZkwa9iPiBO4C3A1uA60Rky5xqnwG+p6rbgWuBv3fLPwKgqr8BvAX4axGxdxHGGPMyKiZ0dwAHVfWwqiaA+4Ar59RRoNqdrwGOuPNbgAcBVHUQGAMWPCtsjDHmzCgm6NuB3rzlPrcs358D7xeRPuCHwB+55XuAd4lIQETWAa8FOuc+gYhcLyLdItIdjUaX+CMYY4xZzHINo1wHfFtVO4B3AP/kDtHcjXNg6Aa+AjwGzPtoqareqapdqtrV1NS0TE0yxhgDxV1e2U9hL7zDLcv3B8AVAKr6SxEJA43ucM0ns5VE5DFg/2m12BhjzJIU06N/AtgkIutEJIRzsvX+OXV6gDcBiMhmIAxERaRcRCrc8rcAKVV9dtlab4wx5qSKugWCe7nkVwA/cLeqfkFEbgW6VfV+9yqcfwQqcU7M/omq/kRE1gI/BjI47wL+QFVfOslzRYFF65xEIzB0GtufKdaupbF2LY21a2m82K41qrrg2PeKu9fN6RKR7hPd76GUrF1LY+1aGmvX0pxr7bJr2o0xxuMs6I0xxuO8GPR3lroBJ2DtWhpr19JYu5bmnGqX58bojTHGFPJij94YY0weC3pjjPE4zwT9yW6lXCoicreIDIrI06VuS5aIdLq3lX5WRJ4RkY+Xuk0AIhIWkV+7t7R+RkRuKXWb8omI370V93+Wui35RORF91bgu0Vk6d/DeYaISK2IfF9EnhORfSJy4Qpo03nu7yn7GBeRT5S6XQAi8kn3df+0iHzXvcPA8uzbC2P07q2U9+PcCrkP59O8162ET+GKyCXAJHCPqr6q1O0BEJE2oE1VnxSRKmAn8O5S/77E+c7HClWdFJEg8CjwcVV9vJTtyhKRT+HcfbVaVX+r1O3JEpEXgS5VXVEfABKR7wCPqOpd7qfqy1V1rMTNynFzox+44GQf5HwZ2tKO83rfoqoxEfke8ENV/fZy7N8rPfpibqVcEqr6MDBS6nbkU9WjqvqkOz8B7GP+HUlfduqYdBeD7mNF9EREpAP4b8BdpW7L2UBEaoBLgG8CqGpiJYW8603AoVKHfJ4AEBGRAFDO7O3eT5tXgr6YWymbBbi3qdgO/KrETQFywyO7gUHgp6q6ItqFcwuQP8G5ncdKo8BPRGSniFxf6sa41gFR4FvucNdd2fterSDXAt8tdSMAVLUfuB3nvmFHgeOq+pPl2r9Xgt6cAhGpBP4N+ISqjpe6PQCqmlbVbTh3Sd0hIiUf7hKR3wIGVXVnqdtyAm9Q1dfgfAvcx9zhwlILAK8Bvu5+89wUsJLOnYWAdwH/Wuq2AIhIHc4oxDpgFVAhIu9frv17JeiLuZWyyeOOgf8bcK+q/nup2zOX+zb/IdzbX5fY63G+QOdFnGHBy0Xk/5S2SbPc3mD2W9x+gDOUWWp9QF/eO7Lv4wT/SvF24ElVHSh1Q1xvBl5Q1aiqJoF/By5arp17JeiLuZWycbknPb8J7FPVvyl1e7JEpElEat35CM7J9edK2ihAVW9W1Q5VXYvz2npQVZett3U6RKTCPaGOOzTyVqDkV3ip6jGgV0TOc4veBJT84og817FChm1cPcDr3Fu7C87va99y7byYLx5Z8VQ1JSI34NwSOXsr5WdK3CwAROS7wGVAo/tVi3+mqt8sbat4PfAB4Cl3PBzg06r6w9I1CYA24Dvu1RA+nC+cX1GXMq5ALcAPnGwgAPyzqv6otE3K+SPgXrfzdRj4cInbA+QOiG8B/rDUbclS1V+JyPeBJ4EUsItlvB2CJy6vNMYYc2JeGboxxhhzAhb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcRb0xhjjcf8foD1eznG40kcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#Dont use this, only here for clean prints\n",
    "\n",
    "method = \"rf\"\n",
    "feature_columns = [\n",
    "    \"SD\",\n",
    "    \"Elev\",\n",
    "    \"Lat\",\n",
    "    \"Long\",\n",
    "    \"DOY\",\n",
    "    \"days without snow\",\n",
    "    \"number frost-defrost\",\n",
    "    \"accum pos degrees\",\n",
    "    \"average age SC\",\n",
    "    \"number layer\",\n",
    "    \"accum solid precip\",\n",
    "    \"accum solid precip in last 10 days\",\n",
    "    \"total precip last 10 days\",\n",
    "    \"average temp last 6 days\",\n",
    "    \"snowclass\"\n",
    "]\n",
    "target = \"SWE\"\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Testing for n_estimators and max depth\n",
    "params = [0, None, 0, 3, 1, True, True]\n",
    "\n",
    "lst = [[]]\n",
    "for i in range(1, 5):\n",
    "    for j in range(1, 10):\n",
    "        results = run_model(method, df, feature_columns, target, test=[], params=[int(j*10), None, int(i*3), 3, 1, True, True])\n",
    "        lst[i-1].append(results[-1])\n",
    "    lst.append([])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for x in lst:\n",
    "    plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-32c5fc1df6c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "method = \"xg\"\n",
    "lst = [[]]\n",
    "for i in range(1, 10):\n",
    "    for j in range(1, 10):\n",
    "        results = run_model(method, df, feature_columns, target, test=[], params=[int(i), int(j*10), 0.035, True])\n",
    "        lst[i-1].append(results[-1])\n",
    "    lst.append([])\n",
    "df = pd.DataFrame(lst)\n",
    "df[df<0] = 0 \n",
    "for x in df:\n",
    "    plt.plot(x)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
